{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0b1170",
   "metadata": {},
   "source": [
    "### Ranker\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\text{IDF}(q_i) \\frac{f(q_i, D) \\times (k_1 + 1)}{f(q_i, D) + k_1 (1 - b + b\\frac{|D|}{\\text{avgdl}})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q$: query  \n",
    "- $D$: $|D|$ is document length  \n",
    "- $\\text{avgdl}$: average length of a document  \n",
    "- $k_1$, $b$: free parameters  \n",
    "- $f(q_i, D)$: the number of times that keyword $q_i$ appears in the document $D$  \n",
    "- $\\text{IDF}(q_i)$: the inverse document frequency  \n",
    "\n",
    "$$\n",
    "\\text{IDF}(q_i) = \\ln(1 + \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$: the number of documents  \n",
    "- $n(q_i)$: the number of documents containing $q_i$\n",
    "\n",
    "[reference](https://www.alexmolas.com/2024/02/05/a-search-engine-in-80-lines.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a6513ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import string\n",
    "from typing import TypeAlias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70d1e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlScores: TypeAlias = dict[str, float]\n",
    "\n",
    "def normalize_string(input_string: str) -> str:\n",
    "    \"\"\"구두점 제거, 소문자 변환, 중복 공백 정리\"\"\"\n",
    "    translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    string_without_punc = input_string.translate(translation_table)\n",
    "    return ' '.join(string_without_punc.split()).lower()\n",
    "\n",
    "\n",
    "def update_url_scores(old: UrlScores, new: UrlScores) -> UrlScores:\n",
    "    \"\"\"새 점수를 기존 딕셔너리에 합산\"\"\"\n",
    "    for url, score in new.items():\n",
    "        old[url] = old.get(url, 0.0) + score\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e81235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        # 역인덱스: 단어 → {URL: 빈도수}\n",
    "        self._index: defaultdict[str, defaultdict[str, int]] = defaultdict(lambda: defaultdict(int))\n",
    "        # 문서 저장: URL → 내용 (길이 계산용)\n",
    "        self._documents: dict[str, str] = {}\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "    @property\n",
    "    def number_of_documents(self) -> int:\n",
    "        return len(self._documents)\n",
    "\n",
    "    @property\n",
    "    def avdl(self) -> float:\n",
    "        \"\"\"전체 문서의 평균 단어 길이\"\"\"\n",
    "        if not self._documents:\n",
    "            return 0.0\n",
    "        total_words = sum(len(content.split()) for content in self._documents.values())\n",
    "        return total_words / self.number_of_documents\n",
    "\n",
    "    def idf(self, kw: str) -> float:\n",
    "        \"\"\"Inverse Document Frequency\"\"\"\n",
    "        N = self.number_of_documents\n",
    "        kw_norm = normalize_string(kw)\n",
    "        n_kw = len(self._index[kw_norm])\n",
    "        return log((N - n_kw + 0.5) / (n_kw + 0.5) + 1)\n",
    "\n",
    "    def bm25(self, kw: str) -> UrlScores:\n",
    "        \"\"\"하나의 키워드에 대한 BM25 점수 계산\"\"\"\n",
    "        kw_norm = normalize_string(kw)\n",
    "        if kw_norm not in self._index:\n",
    "            return {}\n",
    "\n",
    "        result: UrlScores = {}\n",
    "        idf_score = self.idf(kw)\n",
    "        avdl = self.avdl\n",
    "\n",
    "        for url, freq in self._index[kw_norm].items():\n",
    "            doc_length = len(self._documents[url].split())\n",
    "            numerator = freq * (self.k1 + 1)\n",
    "            denominator = freq + self.k1 * (1 - self.b + self.b * doc_length / avdl)\n",
    "            result[url] = idf_score * (numerator / denominator)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def search(self, query: str, top_n: int = 5) -> list[tuple[str, float]]:\n",
    "        \"\"\"쿼리 검색 → 상위 top_n 결과 반환 (점수 내림차순)\"\"\"\n",
    "        keywords = normalize_string(query).split()\n",
    "        url_scores: UrlScores = {}\n",
    "\n",
    "        for kw in keywords:\n",
    "            kw_scores = self.bm25(kw)\n",
    "            update_url_scores(url_scores, kw_scores)\n",
    "\n",
    "        ranked = sorted(url_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_n]\n",
    "\n",
    "    def index(self, url: str, content: str) -> None:\n",
    "        \"\"\"단일 문서 인덱싱\"\"\"\n",
    "        self._documents[url] = content\n",
    "        words = normalize_string(content).split()\n",
    "        for word in words:\n",
    "            self._index[word][url] += 1\n",
    "\n",
    "    def bulk_index(self, documents: list[tuple[str, str]]) -> None:\n",
    "        \"\"\"여러 문서 일괄 인덱싱\"\"\"\n",
    "        for url, content in documents:\n",
    "            self.index(url, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14863015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱싱된 문서: 4개\n",
      "평균 문서 길이: 10.25 단어\n",
      "\n",
      "1. https://example.com/post1 (점수: 1.3126)\n",
      "2. https://example.com/post4 (점수: 0.1229)\n",
      "3. https://example.com/post2 (점수: 0.1115)\n",
      "4. https://example.com/post3 (점수: 0.0905)\n"
     ]
    }
   ],
   "source": [
    "# 검색 엔진 생성\n",
    "engine = SearchEngine()\n",
    "\n",
    "# 샘플 문서\n",
    "sample_docs = [\n",
    "    (\"https://example.com/post1\", \"Python is a great programming language. Python is easy to learn.\"),\n",
    "    (\"https://example.com/post2\", \"Machine learning with Python is very popular these days.\"),\n",
    "    (\"https://example.com/post3\", \"JavaScript is used for web development. Python can also be used on the web.\"),\n",
    "    (\"https://example.com/post4\", \"I love coding in Python every day.\"),\n",
    "]\n",
    "\n",
    "engine.bulk_index(sample_docs)\n",
    "\n",
    "print(f\"인덱싱된 문서: {engine.number_of_documents}개\")\n",
    "print(f\"평균 문서 길이: {engine.avdl:.2f} 단어\\n\")\n",
    "\n",
    "# 검색 테스트\n",
    "results = engine.search(\"Python programming\", top_n=4)\n",
    "for rank, (url, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. {url} (점수: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfb1c7",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "[wikidocs](https://wikidocs.net/31698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "646615de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c119837",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = {\n",
    "    'a', 'an', 'and', 'the', 'or', 'of', 'to', 'in', 'for', 'on', 'with', 'at', 'by',\n",
    "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
    "    'my', 'your', 'his', 'its', 'our', 'their', 'this', 'that', 'these', 'those',\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do',\n",
    "    'does', 'did', 'will', 'would', 'shall', 'should', 'may', 'might', 'must',\n",
    "    'can', 'could', 'as', 'but', 'if', 'because', 'until', 'while', 'about',\n",
    "    'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "    'below', 'from', 'up', 'down', 'out', 'off', 'over', 'under', 'again', 'further',\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "    'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',\n",
    "    'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very'\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in STOP_WORDS and token.strip() != '']\n",
    "    tokens = [stem_token(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def stem_token(token):\n",
    "    suffixes = ['ing', 'ly', 'ed', 'es', 's']\n",
    "    for suffix in suffixes:\n",
    "        if token.endswith(suffix) and len(token) > len(suffix) + 1:\n",
    "            token = token[:-len(suffix)]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8b53c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(documents, index_file='inverted_index.csv'):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        tokens = preprocess_text(doc)\n",
    "        term_positions = defaultdict(list)\n",
    "        for pos, term in enumerate(tokens):\n",
    "            term_positions[term].append(pos)\n",
    "        for term, positions in term_positions.items():\n",
    "            inverted_index[term].append((doc_id, positions))\n",
    "    \n",
    "    # CSV 저장\n",
    "    with open(index_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['term', 'doc_id', 'positions'])\n",
    "        for term, doc_info in inverted_index.items():\n",
    "            for doc_id, positions in doc_info:\n",
    "                writer.writerow([term, doc_id, str(positions)])\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39e7a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(documents, inverted_index):\n",
    "    num_docs = len(documents)\n",
    "    tfidf = defaultdict(dict)\n",
    "    doc_lengths = [len(preprocess_text(doc)) for doc in documents]\n",
    "    doc_freq = {term: len(entries) for term, entries in inverted_index.items()}\n",
    "    \n",
    "    for term, entries in inverted_index.items():\n",
    "        idf = math.log(num_docs / (doc_freq[term] + 1))\n",
    "        for doc_id, positions in entries:\n",
    "            tf = len(positions) / doc_lengths[doc_id] if doc_lengths[doc_id] > 0 else 0\n",
    "            tfidf[doc_id][term] = tf * idf\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85cca302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, documents, inverted_index, tfidf_scores, top_n=5):\n",
    "    query_terms = preprocess_text(query)\n",
    "    if not query_terms:\n",
    "        return []\n",
    "    \n",
    "    relevant_docs = set()\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            for doc_id, _ in inverted_index[term]:\n",
    "                relevant_docs.add(doc_id)\n",
    "    \n",
    "    scores = []\n",
    "    for doc_id in relevant_docs:\n",
    "        score = sum(tfidf_scores.get(doc_id, {}).get(term, 0) for term in query_terms)\n",
    "        score /= len(query_terms) or 1\n",
    "        scores.append((doc_id, score))\n",
    "    \n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    results = [(documents[doc_id], score) for doc_id, score in scores[:top_n] if score > 0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81d170c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Score: 0.0422 - Building a search engine involves crawling, indexing, and querying large datasets.\n",
      "2. Score: 0.0339 - Python is a versatile programming language used for web development and data analysis.\n",
      "3. Score: 0.0093 - Bing integrates AI to improve search accuracy and user experience.\n",
      "4. Score: 0.0068 - Search engines like Google use complex ranking systems to deliver relevant results.\n"
     ]
    }
   ],
   "source": [
    "# 샘플 문서 (이전처럼 위키나 블로그에서 가져온 텍스트로 대체 가능)\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data analysis.\",\n",
    "    \"Machine learning algorithms can predict outcomes based on historical data.\",\n",
    "    \"Search engines like Google use complex ranking systems to deliver relevant results.\",\n",
    "    \"Bing integrates AI to improve search accuracy and user experience.\",\n",
    "    \"Building a search engine involves crawling, indexing, and querying large datasets.\"\n",
    "]\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "tfidf_scores = calculate_tfidf(documents, inverted_index)\n",
    "\n",
    "# 검색 테스트\n",
    "results = search(\"search engine python\", documents, inverted_index, tfidf_scores)\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"{i}. Score: {score:.4f} - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30fcd4",
   "metadata": {},
   "source": [
    "### Scrap sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "806241de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting automatic crawl from: https://www.paulgraham.com/articles.html\n",
      "Domain restricted to: www.paulgraham.com\n",
      "Max pages: 30, Max depth: 3\n",
      "\n",
      "[1/30] Crawling: https://www.paulgraham.com/articles.html (depth: 0)\n",
      "> Indexed successfully | Found 230 new links\n",
      "[2/30] Crawling: https://www.paulgraham.com/index.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[3/30] Crawling: https://www.paulgraham.com/greatwork.html (depth: 1)\n",
      "> Indexed successfully | Found 29 new links\n",
      "[4/30] Crawling: https://www.paulgraham.com/kids.html (depth: 1)\n",
      "> Indexed successfully | Found 1 new links\n",
      "[5/30] Crawling: https://www.paulgraham.com/selfindulgence.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[6/30] Crawling: https://www.paulgraham.com/field.html (depth: 1)\n",
      "> Indexed successfully | Found 1 new links\n",
      "[7/30] Crawling: https://www.paulgraham.com/goodwriting.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[8/30] Crawling: https://www.paulgraham.com/do.html (depth: 1)\n",
      "> Indexed successfully | Found 2 new links\n",
      "[9/30] Crawling: https://www.paulgraham.com/woke.html (depth: 1)\n",
      "> Indexed successfully | Found 19 new links\n",
      "[10/30] Crawling: https://www.paulgraham.com/writes.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[11/30] Crawling: https://www.paulgraham.com/when.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[12/30] Crawling: https://www.paulgraham.com/foundermode.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[13/30] Crawling: https://www.paulgraham.com/persistence.html (depth: 1)\n",
      "> Indexed successfully | Found 6 new links\n",
      "[14/30] Crawling: https://www.paulgraham.com/reddits.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[15/30] Crawling: https://www.paulgraham.com/google.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[16/30] Crawling: https://www.paulgraham.com/best.html (depth: 1)\n",
      "> Indexed successfully | Found 12 new links\n",
      "[17/30] Crawling: https://www.paulgraham.com/superlinear.html (depth: 1)\n",
      "> Indexed successfully | Found 12 new links\n",
      "[18/30] Crawling: https://www.paulgraham.com/getideas.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[19/30] Crawling: https://www.paulgraham.com/read.html (depth: 1)\n",
      "> Indexed successfully | Found 2 new links\n",
      "[20/30] Crawling: https://www.paulgraham.com/want.html (depth: 1)\n",
      "> Indexed successfully | Found 2 new links\n",
      "[21/30] Crawling: https://www.paulgraham.com/alien.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[22/30] Crawling: https://www.paulgraham.com/users.html (depth: 1)\n",
      "> Indexed successfully | Found 5 new links\n",
      "[23/30] Crawling: https://www.paulgraham.com/heresy.html (depth: 1)\n",
      "> Indexed successfully | Found 6 new links\n",
      "[24/30] Crawling: https://www.paulgraham.com/words.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[25/30] Crawling: https://www.paulgraham.com/goodtaste.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[26/30] Crawling: https://www.paulgraham.com/smart.html (depth: 1)\n",
      "> Indexed successfully | Found 4 new links\n",
      "[27/30] Crawling: https://www.paulgraham.com/weird.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[28/30] Crawling: https://www.paulgraham.com/hwh.html (depth: 1)\n",
      "> Indexed successfully | Found 9 new links\n",
      "[29/30] Crawling: https://www.paulgraham.com/own.html (depth: 1)\n",
      "> Indexed successfully | Found 5 new links\n",
      "[30/30] Crawling: https://www.paulgraham.com/fn.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "\n",
      "Crawling finished! Total indexed pages: 30\n",
      "Your search engine is now ready to query these pages!\n",
      "\n",
      "============================================================\n",
      "1. [5.4732] https://www.paulgraham.com/articles.html\n",
      "2. [4.7139] https://www.paulgraham.com/users.html\n",
      "3. [4.0784] https://www.paulgraham.com/reddits.html\n",
      "4. [2.0999] https://www.paulgraham.com/google.html\n",
      "5. [1.8857] https://www.paulgraham.com/superlinear.html\n",
      "\n",
      "\n",
      "1. [7.5700] https://www.paulgraham.com/articles.html\n",
      "2. [6.8176] https://www.paulgraham.com/weird.html\n",
      "3. [4.0169] https://www.paulgraham.com/google.html\n",
      "4. [3.9901] https://www.paulgraham.com/own.html\n",
      "5. [3.6844] https://www.paulgraham.com/reddits.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def is_valid_url(url: str, base_domain: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a URL is valid and belongs to the same domain as the start URL.\n",
    "    Also skips non-HTML files like images, PDFs, etc.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return (\n",
    "        parsed.scheme in ('http', 'https') and\n",
    "        parsed.netloc == base_domain and\n",
    "        not parsed.path.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.js', '.css'))\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_content(html: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract clean, readable text from HTML.\n",
    "    Removes scripts, styles, navigation, footers, etc., and skips pages with too little content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements that usually don't contain useful content\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\", \"iframe\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Skip pages with very little text (e.g., login pages, error pages)\n",
    "    if len(text.split()) < 20:\n",
    "        return None\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def auto_crawl_and_index(\n",
    "    engine,\n",
    "    start_url: str,\n",
    "    max_pages: int = 50,\n",
    "    max_depth: int = 5,\n",
    "    delay: float = 1.0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fully automatic web crawler with indexing.\n",
    "    Starts from a seed URL and follows links breadth-first (BFS), just like Google.\n",
    "    Only crawls within the same domain and respects basic politeness rules.\n",
    "    \"\"\"\n",
    "    # Ensure URL has protocol\n",
    "    if not start_url.startswith(('http://', 'https://')):\n",
    "        start_url = 'https://' + start_url\n",
    "    \n",
    "    base_domain = urlparse(start_url).netloc\n",
    "    visited = set()\n",
    "    queue = deque([(start_url, 0)])  # (url, current_depth)\n",
    "\n",
    "    print(f\"Starting automatic crawl from: {start_url}\")\n",
    "    print(f\"Domain restricted to: {base_domain}\")\n",
    "    print(f\"Max pages: {max_pages}, Max depth: {max_depth}\\n\")\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        current_url, depth = queue.popleft()\n",
    "\n",
    "        # Skip if already visited or too deep\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        print(f\"[{len(visited)+1}/{max_pages}] Crawling: {current_url} (depth: {depth})\")\n",
    "\n",
    "        try:\n",
    "            # Polite User-Agent (important to avoid being blocked)\n",
    "            headers = {\n",
    "                'User-Agent': 'MyPersonalSearchBot/1.0 (+https://yourwebsite.com/bot-info)'\n",
    "            }\n",
    "            response = requests.get(current_url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Skip non-HTML content\n",
    "            content_type = response.headers.get('Content-Type', '')\n",
    "            if 'text/html' not in content_type:\n",
    "                print(f\"Not HTML, skipping\")\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            content = clean_content(response.text)\n",
    "            if content is None:\n",
    "                print(f\"  → Too little content, skipping\")\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            # Index the page into your SearchEngine\n",
    "            engine.index(current_url, content)\n",
    "            visited.add(current_url)\n",
    "\n",
    "            # Extract and enqueue new links\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            new_links = 0\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                next_url = urljoin(current_url, link['href'])\n",
    "\n",
    "                # Add only valid, unvisited internal links\n",
    "                if (is_valid_url(next_url, base_domain) and\n",
    "                    next_url not in visited and\n",
    "                    next_url not in [item[0] for item in queue]):\n",
    "                    \n",
    "                    queue.append((next_url, depth + 1))\n",
    "                    new_links += 1\n",
    "\n",
    "            print(f\"> Indexed successfully | Found {new_links} new links\")\n",
    "\n",
    "            # Be respectful: delay between requests\n",
    "            time.sleep(delay)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    print(f\"\\nCrawling finished! Total indexed pages: {len(visited)}\")\n",
    "    print(\"Your search engine is now ready to query these pages!\")\n",
    "\n",
    "\n",
    "# ==================== Example Usage ====================\n",
    "\n",
    "# Create your SearchEngine instance (make sure the class is defined earlier)\n",
    "engine = SearchEngine()\n",
    "\n",
    "# Start crawling from a seed page\n",
    "# Great test sites: small blogs, essay collections, documentation sites\n",
    "auto_crawl_and_index(\n",
    "    engine,\n",
    "    start_url=\"https://www.paulgraham.com/articles.html\",  # Paul Graham's essays (excellent content)\n",
    "    max_pages=30,\n",
    "    max_depth=3,\n",
    "    delay=1.5  # Be polite – 1.5 seconds between requests\n",
    ")\n",
    "\n",
    "# Test searches\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "results = engine.search(\"startup funding\", top_n=5)\n",
    "for rank, (url, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. [{score:.4f}] {url}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "results = engine.search(\"programming language design\")\n",
    "for rank, (url, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. [{score:.4f}] {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a432a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Deep crawl of one site\n",
      "DOMAIN-LOCKED MODE: Limited to 1 starting domain(s)\n",
      "Seed URLs: ['https://www.paulgraham.com/articles.html']\n",
      "Max pages: 40 | Max depth: 4 | Delay: 1.5s\n",
      "\n",
      "[1/40] Crawling: https://www.paulgraham.com/articles.html (depth: 0)\n",
      "Indexed | +230 new links queued\n",
      "[2/40] Crawling: https://www.paulgraham.com/index.html (depth: 1)\n",
      "[3/40] Crawling: https://www.paulgraham.com/greatwork.html (depth: 1)\n",
      "Indexed | +29 new links queued\n",
      "[4/40] Crawling: https://www.paulgraham.com/kids.html (depth: 1)\n",
      "Indexed | +1 new links queued\n",
      "[5/40] Crawling: https://www.paulgraham.com/selfindulgence.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[6/40] Crawling: https://www.paulgraham.com/field.html (depth: 1)\n",
      "Indexed | +1 new links queued\n",
      "[7/40] Crawling: https://www.paulgraham.com/goodwriting.html (depth: 1)\n",
      "Indexed | +3 new links queued\n",
      "[8/40] Crawling: https://www.paulgraham.com/do.html (depth: 1)\n",
      "Indexed | +2 new links queued\n",
      "[9/40] Crawling: https://www.paulgraham.com/woke.html (depth: 1)\n",
      "Indexed | +19 new links queued\n",
      "[10/40] Crawling: https://www.paulgraham.com/writes.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[11/40] Crawling: https://www.paulgraham.com/when.html (depth: 1)\n",
      "Indexed | +3 new links queued\n",
      "[12/40] Crawling: https://www.paulgraham.com/foundermode.html (depth: 1)\n",
      "Indexed | +3 new links queued\n",
      "[13/40] Crawling: https://www.paulgraham.com/persistence.html (depth: 1)\n",
      "Indexed | +6 new links queued\n",
      "[14/40] Crawling: https://www.paulgraham.com/reddits.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[15/40] Crawling: https://www.paulgraham.com/google.html (depth: 1)\n",
      "Indexed | +3 new links queued\n",
      "[16/40] Crawling: https://www.paulgraham.com/best.html (depth: 1)\n",
      "Indexed | +12 new links queued\n",
      "[17/40] Crawling: https://www.paulgraham.com/superlinear.html (depth: 1)\n",
      "Indexed | +12 new links queued\n",
      "[18/40] Crawling: https://www.paulgraham.com/getideas.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[19/40] Crawling: https://www.paulgraham.com/read.html (depth: 1)\n",
      "Indexed | +2 new links queued\n",
      "[20/40] Crawling: https://www.paulgraham.com/want.html (depth: 1)\n",
      "Indexed | +2 new links queued\n",
      "[21/40] Crawling: https://www.paulgraham.com/alien.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[22/40] Crawling: https://www.paulgraham.com/users.html (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[23/40] Crawling: https://www.paulgraham.com/heresy.html (depth: 1)\n",
      "Indexed | +6 new links queued\n",
      "[24/40] Crawling: https://www.paulgraham.com/words.html (depth: 1)\n",
      "Indexed | +3 new links queued\n",
      "[25/40] Crawling: https://www.paulgraham.com/goodtaste.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[26/40] Crawling: https://www.paulgraham.com/smart.html (depth: 1)\n",
      "Indexed | +4 new links queued\n",
      "[27/40] Crawling: https://www.paulgraham.com/weird.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[28/40] Crawling: https://www.paulgraham.com/hwh.html (depth: 1)\n",
      "Indexed | +9 new links queued\n",
      "[29/40] Crawling: https://www.paulgraham.com/own.html (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[30/40] Crawling: https://www.paulgraham.com/fn.html (depth: 1)\n",
      "Indexed | +3 new links queued\n",
      "[31/40] Crawling: https://www.paulgraham.com/newideas.html (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[32/40] Crawling: https://www.paulgraham.com/nft.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[33/40] Crawling: https://www.paulgraham.com/real.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[34/40] Crawling: https://www.paulgraham.com/richnow.html (depth: 1)\n",
      "Indexed | +9 new links queued\n",
      "[35/40] Crawling: https://www.paulgraham.com/simply.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "[36/40] Crawling: https://www.paulgraham.com/donate.html (depth: 1)\n",
      "Indexed | +1 new links queued\n",
      "[37/40] Crawling: https://www.paulgraham.com/worked.html (depth: 1)\n",
      "Indexed | +20 new links queued\n",
      "[38/40] Crawling: https://www.paulgraham.com/earnest.html (depth: 1)\n",
      "Indexed | +6 new links queued\n",
      "[39/40] Crawling: https://www.paulgraham.com/ace.html (depth: 1)\n",
      "Indexed | +4 new links queued\n",
      "[40/40] Crawling: https://www.paulgraham.com/airbnbs.html (depth: 1)\n",
      "Indexed | +0 new links queued\n",
      "\n",
      "Crawling complete! Indexed 40 pages.\n",
      "Your personal search engine now knows more of the web!\n",
      "\n",
      "======================================================================\n",
      "Example 2: GLOBAL MODE - Exploring the open web\n",
      "GLOBAL MODE ACTIVATED: Will discover and crawl new domains!\n",
      "Seed URLs: ['https://news.ycombinator.com', 'https://www.lesswrong.com', 'https://www.paulgraham.com']\n",
      "Max pages: 80 | Max depth: 5 | Delay: 2.0s\n",
      "\n",
      "[1/80] Crawling: https://news.ycombinator.com (depth: 0)\n",
      "Indexed | +197 new links queued\n",
      "[2/80] Crawling: https://www.lesswrong.com (depth: 0)\n",
      "Indexed | +93 new links queued\n",
      "[3/80] Crawling: https://www.paulgraham.com (depth: 0)\n",
      "[4/80] Crawling: https://news.ycombinator.com/news (depth: 1)\n",
      "Indexed | +1 new links queued\n",
      "[5/80] Crawling: https://news.ycombinator.com/newest (depth: 1)\n",
      "Indexed | +195 new links queued\n",
      "[6/80] Crawling: https://news.ycombinator.com/front (depth: 1)\n",
      "Indexed | +100 new links queued\n",
      "[7/80] Crawling: https://news.ycombinator.com/newcomments (depth: 1)\n",
      "Indexed | +148 new links queued\n",
      "[8/80] Crawling: https://news.ycombinator.com/ask (depth: 1)\n",
      "Indexed | +91 new links queued\n",
      "[9/80] Crawling: https://news.ycombinator.com/show (depth: 1)\n",
      "Indexed | +135 new links queued\n",
      "[10/80] Crawling: https://news.ycombinator.com/jobs (depth: 1)\n",
      "Indexed | +68 new links queued\n",
      "[11/80] Crawling: https://news.ycombinator.com/submit (depth: 1)\n",
      "[12/80] Crawling: https://news.ycombinator.com/login?goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/login?goto=news\n",
      "[13/80] Crawling: https://news.ycombinator.com/vote?id=46341305&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46341305&how=up&goto=news\n",
      "[14/80] Crawling: https://haveibeenflocked.com/news/cyble-downtime (depth: 1)\n",
      "[15/80] Crawling: https://news.ycombinator.com/from?site=haveibeenflocked.com (depth: 1)\n",
      "Indexed | +9 new links queued\n",
      "[16/80] Crawling: https://news.ycombinator.com/user?id=_a9 (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[17/80] Crawling: https://news.ycombinator.com/item?id=46341305 (depth: 1)\n",
      "Indexed | +348 new links queued\n",
      "[18/80] Crawling: https://news.ycombinator.com/hide?id=46341305&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46341305&goto=news\n",
      "[19/80] Crawling: https://news.ycombinator.com/vote?id=46339600&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46339600&how=up&goto=news\n",
      "[20/80] Crawling: https://www.jmail.world (depth: 1)\n",
      "NEW DOMAIN DISCOVERED: www.jmail.world\n",
      "Indexed | +34 new links queued\n",
      "[21/80] Crawling: https://news.ycombinator.com/from?site=jmail.world (depth: 1)\n",
      "Indexed | +21 new links queued\n",
      "[22/80] Crawling: https://news.ycombinator.com/user?id=lukeigel (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[23/80] Crawling: https://news.ycombinator.com/item?id=46339600 (depth: 1)\n",
      "Indexed | +627 new links queued\n",
      "[24/80] Crawling: https://news.ycombinator.com/hide?id=46339600&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46339600&goto=news\n",
      "[25/80] Crawling: https://news.ycombinator.com/vote?id=46342166&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46342166&how=up&goto=news\n",
      "[26/80] Crawling: https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/ (depth: 1)\n",
      "NEW DOMAIN DISCOVERED: metr.org\n",
      "Indexed | +45 new links queued\n",
      "[27/80] Crawling: https://news.ycombinator.com/from?site=metr.org (depth: 1)\n",
      "Indexed | +88 new links queued\n",
      "[28/80] Crawling: https://news.ycombinator.com/user?id=spicypete (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[29/80] Crawling: https://news.ycombinator.com/item?id=46342166 (depth: 1)\n",
      "Indexed | +358 new links queued\n",
      "[30/80] Crawling: https://news.ycombinator.com/hide?id=46342166&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46342166&goto=news\n",
      "[31/80] Crawling: https://news.ycombinator.com/vote?id=46338339&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46338339&how=up&goto=news\n",
      "[32/80] Crawling: https://annas-archive.li/blog/backing-up-spotify.html (depth: 1)\n",
      "NEW DOMAIN DISCOVERED: annas-archive.li\n",
      "Indexed | +13 new links queued\n",
      "[33/80] Crawling: https://news.ycombinator.com/from?site=annas-archive.li (depth: 1)\n",
      "Indexed | +13 new links queued\n",
      "[34/80] Crawling: https://news.ycombinator.com/user?id=vitplister (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[35/80] Crawling: https://news.ycombinator.com/item?id=46338339 (depth: 1)\n",
      "Indexed | +1740 new links queued\n",
      "[36/80] Crawling: https://news.ycombinator.com/hide?id=46338339&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46338339&goto=news\n",
      "[37/80] Crawling: https://news.ycombinator.com/vote?id=46342447&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46342447&how=up&goto=news\n",
      "[38/80] Crawling: https://www.ucsf.edu/news/2025/12/431206/indoor-tanning-makes-youthful-skin-much-older-genetic-level (depth: 1)\n",
      "NEW DOMAIN DISCOVERED: www.ucsf.edu\n",
      "Indexed | +93 new links queued\n",
      "[39/80] Crawling: https://news.ycombinator.com/from?site=ucsf.edu (depth: 1)\n",
      "Indexed | +132 new links queued\n",
      "[40/80] Crawling: https://news.ycombinator.com/user?id=SanjayMehta (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[41/80] Crawling: https://news.ycombinator.com/item?id=46342447 (depth: 1)\n",
      "Indexed | +38 new links queued\n",
      "[42/80] Crawling: https://news.ycombinator.com/hide?id=46342447&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46342447&goto=news\n",
      "[43/80] Crawling: https://news.ycombinator.com/vote?id=46342859&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46342859&how=up&goto=news\n",
      "[44/80] Crawling: https://www.ruby-lang.org/en/ (depth: 1)\n",
      "Could not fetch robots.txt for https://www.ruby-lang.org/: HTTP Error 404: Not Found\n",
      "Blocked by robots.txt: https://www.ruby-lang.org/en/\n",
      "[45/80] Crawling: https://news.ycombinator.com/from?site=ruby-lang.org (depth: 1)\n",
      "Indexed | +130 new links queued\n",
      "[46/80] Crawling: https://news.ycombinator.com/user?id=psxuaw (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[47/80] Crawling: https://news.ycombinator.com/item?id=46342859 (depth: 1)\n",
      "Indexed | +62 new links queued\n",
      "[48/80] Crawling: https://news.ycombinator.com/hide?id=46342859&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46342859&goto=news\n",
      "[49/80] Crawling: https://news.ycombinator.com/vote?id=46342528&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46342528&how=up&goto=news\n",
      "[50/80] Crawling: https://lareviewofbooks.org/article/isengard-in-oxford/ (depth: 1)\n",
      "NEW DOMAIN DISCOVERED: lareviewofbooks.org\n",
      "Indexed | +49 new links queued\n",
      "[51/80] Crawling: https://news.ycombinator.com/from?site=lareviewofbooks.org (depth: 1)\n",
      "Indexed | +128 new links queued\n",
      "[52/80] Crawling: https://news.ycombinator.com/user?id=lermontov (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[53/80] Crawling: https://news.ycombinator.com/item?id=46342528 (depth: 1)\n",
      "Indexed | +4 new links queued\n",
      "[54/80] Crawling: https://news.ycombinator.com/hide?id=46342528&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46342528&goto=news\n",
      "[55/80] Crawling: https://news.ycombinator.com/vote?id=46339031&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46339031&how=up&goto=news\n",
      "[56/80] Crawling: https://www.bbc.com/news/articles/cj4qzgvxxgvo (depth: 1)\n",
      "NEW DOMAIN DISCOVERED: www.bbc.com\n",
      "Indexed | +154 new links queued\n",
      "[57/80] Crawling: https://news.ycombinator.com/from?site=bbc.com (depth: 1)\n",
      "Indexed | +131 new links queued\n",
      "[58/80] Crawling: https://news.ycombinator.com/user?id=1659447091 (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[59/80] Crawling: https://news.ycombinator.com/item?id=46339031 (depth: 1)\n",
      "Indexed | +378 new links queued\n",
      "[60/80] Crawling: https://news.ycombinator.com/hide?id=46339031&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46339031&goto=news\n",
      "[61/80] Crawling: https://news.ycombinator.com/vote?id=46339777&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46339777&how=up&goto=news\n",
      "[62/80] Crawling: https://claude.com/chrome (depth: 1)\n",
      "Could not fetch robots.txt for https://claude.com/: HTTP Error 403: Forbidden\n",
      "Blocked by robots.txt: https://claude.com/chrome\n",
      "[63/80] Crawling: https://news.ycombinator.com/from?site=claude.com (depth: 1)\n",
      "Indexed | +138 new links queued\n",
      "[64/80] Crawling: https://news.ycombinator.com/user?id=ianrahman (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[65/80] Crawling: https://news.ycombinator.com/item?id=46339777 (depth: 1)\n",
      "Indexed | +407 new links queued\n",
      "[66/80] Crawling: https://news.ycombinator.com/hide?id=46339777&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46339777&goto=news\n",
      "[67/80] Crawling: https://news.ycombinator.com/vote?id=46342950&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46342950&how=up&goto=news\n",
      "[68/80] Crawling: https://www.earthasweknowit.com/pages/inca_construction (depth: 1)\n",
      "NEW DOMAIN DISCOVERED: www.earthasweknowit.com\n",
      "Indexed | +128 new links queued\n",
      "[69/80] Crawling: https://news.ycombinator.com/from?site=earthasweknowit.com (depth: 1)\n",
      "Indexed | +7 new links queued\n",
      "[70/80] Crawling: https://news.ycombinator.com/user?id=jppope (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[71/80] Crawling: https://news.ycombinator.com/item?id=46342950 (depth: 1)\n",
      "Indexed | +8 new links queued\n",
      "[72/80] Crawling: https://news.ycombinator.com/hide?id=46342950&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46342950&goto=news\n",
      "[73/80] Crawling: https://news.ycombinator.com/vote?id=46337438&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46337438&how=up&goto=news\n",
      "[74/80] Crawling: https://www.a1k0n.net/2025/12/19/tiny-tapeout-demo.html (depth: 1)\n",
      "NEW DOMAIN DISCOVERED: www.a1k0n.net\n",
      "Indexed | +41 new links queued\n",
      "[75/80] Crawling: https://news.ycombinator.com/from?site=a1k0n.net (depth: 1)\n",
      "Indexed | +118 new links queued\n",
      "[76/80] Crawling: https://news.ycombinator.com/user?id=a1k0n (depth: 1)\n",
      "Indexed | +5 new links queued\n",
      "[77/80] Crawling: https://news.ycombinator.com/item?id=46337438 (depth: 1)\n",
      "Indexed | +223 new links queued\n",
      "[78/80] Crawling: https://news.ycombinator.com/hide?id=46337438&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46337438&goto=news\n",
      "[79/80] Crawling: https://news.ycombinator.com/vote?id=46301059&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46301059&how=up&goto=news\n",
      "[80/80] Crawling: https://utcc.utoronto.ca/~cks/space/blog/programming/ErrorsShouldRequireFixing (depth: 1)\n",
      "NEW DOMAIN DISCOVERED: utcc.utoronto.ca\n",
      "Indexed | +35 new links queued\n",
      "\n",
      "Crawling complete! Indexed 80 pages.\n",
      "Discovered 12 different domains.\n",
      "Your personal search engine now knows more of the web!\n",
      "\n",
      "======================================================================\n",
      "1. [10.5588] https://news.ycombinator.com/from?site=lareviewofbooks.org\n",
      "2. [8.3141] https://news.ycombinator.com/item?id=46341305\n",
      "3. [5.9282] https://www.paulgraham.com/smart.html\n",
      "4. [5.6748] https://www.ucsf.edu/news/2025/12/431206/indoor-tanning-makes-youthful-skin-much-older-genetic-level\n",
      "5. [4.5030] https://news.ycombinator.com/newest\n",
      "1. [6.9064] https://www.paulgraham.com/google.html\n",
      "2. [5.1293] https://news.ycombinator.com/item?id=46339031\n",
      "3. [4.6351] https://www.paulgraham.com/weird.html\n",
      "4. [3.9476] https://utcc.utoronto.ca/~cks/space/blog/programming/ErrorsShouldRequireFixing\n",
      "5. [3.8675] https://www.paulgraham.com/worked.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "from typing import Set\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "def is_valid_url(url: str, allowed_domains: Set[str] | None = None) -> bool:\n",
    "    \"\"\"\n",
    "    Validate URL: proper scheme, not a file, and (optionally) within allowed domains.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.scheme not in ('http', 'https'):\n",
    "        return False\n",
    "    if parsed.path.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.js', '.css', '.ico')):\n",
    "        return False\n",
    "    if allowed_domains is not None and parsed.netloc not in allowed_domains:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_content(html: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract clean readable text from HTML.\n",
    "    Removes noise (scripts, nav, footer, etc.) and skips pages with too little content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Remove non-content elements\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\", \n",
    "                     \"iframe\", \"noscript\", \"form\", \"button\", \"input\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Skip pages with almost no real content\n",
    "    if len(text.split()) < 30:\n",
    "        return None\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def get_robots_parser(domain: str, user_agent: str) -> RobotFileParser:\n",
    "    \"\"\"\n",
    "    Fetch and parse robots.txt for a given domain.\n",
    "    Returns a RobotFileParser object.\n",
    "    \"\"\"\n",
    "    rp = RobotFileParser()\n",
    "    robots_url = urllib.parse.urljoin(domain, '/robots.txt')\n",
    "    \n",
    "    try:\n",
    "        with urlopen(robots_url, timeout=10) as response:\n",
    "            rp.parse(response.read().decode('utf-8').splitlines())\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch robots.txt for {domain}: {e}\")\n",
    "        # If robots.txt is unavailable, assume all is allowed (common practice)\n",
    "    \n",
    "    return rp\n",
    "\n",
    "\n",
    "def auto_crawl_and_index(\n",
    "    engine,\n",
    "    seed_urls,\n",
    "    max_pages: int = 100,\n",
    "    max_depth: int = 6,\n",
    "    delay: float = 1.0,\n",
    "    global_mode: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Advanced automatic web crawler with two modes:\n",
    "    \n",
    "    - global_mode=False (default): Crawl within the initial domains only (deep dive one site)\n",
    "    - global_mode=True: Allow discovering and crawling NEW domains (true web-wide search engine behavior)\n",
    "    \n",
    "    Respects robots.txt for each domain.\n",
    "    \"\"\"\n",
    "    # Convert single string to list if needed\n",
    "    if isinstance(seed_urls, str):\n",
    "        seed_urls = [seed_urls]\n",
    "    \n",
    "    # Normalize seed URLs\n",
    "    normalized_seeds = []\n",
    "    for url in seed_urls:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url\n",
    "        normalized_seeds.append(url)\n",
    "    \n",
    "    seed_urls = normalized_seeds\n",
    "    \n",
    "    # Determine allowed domains\n",
    "    if global_mode:\n",
    "        allowed_domains = None  # No restriction - can explore the entire web\n",
    "        print(\"GLOBAL MODE ACTIVATED: Will discover and crawl new domains!\")\n",
    "    else:\n",
    "        allowed_domains = {urlparse(url).netloc for url in seed_urls}\n",
    "        print(f\"DOMAIN-LOCKED MODE: Limited to {len(allowed_domains)} starting domain(s)\")\n",
    "    \n",
    "    print(f\"Seed URLs: {seed_urls}\")\n",
    "    print(f\"Max pages: {max_pages} | Max depth: {max_depth} | Delay: {delay}s\\n\")\n",
    "\n",
    "    visited = set()\n",
    "    queue = deque([(url, 0) for url in seed_urls])  # (url, depth)\n",
    "\n",
    "    discovered_domains = set(urlparse(url).netloc for url in seed_urls)\n",
    "    robots_parsers = {}  # Cache: domain -> RobotFileParser\n",
    "\n",
    "    user_agent = 'MyGlobalSearchBot/1.0 (+https://yourwebsite.com/bot)'\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        current_url, depth = queue.popleft()\n",
    "\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        print(f\"[{len(visited)+1}/{max_pages}] Crawling: {current_url} (depth: {depth})\")\n",
    "\n",
    "        # Get domain and check robots.txt\n",
    "        parsed_url = urlparse(current_url)\n",
    "        domain = f\"{parsed_url.scheme}://{parsed_url.netloc}/\"\n",
    "        \n",
    "        if domain not in robots_parsers:\n",
    "            robots_parsers[domain] = get_robots_parser(domain, user_agent)\n",
    "        \n",
    "        rp = robots_parsers[domain]\n",
    "        \n",
    "        if not rp.can_fetch(user_agent, current_url):\n",
    "            print(f\"Blocked by robots.txt: {current_url}\")\n",
    "            visited.add(current_url)  # Mark as visited to avoid retrying\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            headers = {'User-Agent': user_agent}\n",
    "            response = requests.get(current_url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            content = clean_content(response.text)\n",
    "            if content is None:\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            # Index the page\n",
    "            engine.index(current_url, content)\n",
    "            visited.add(current_url)\n",
    "\n",
    "            current_domain = parsed_url.netloc\n",
    "            if current_domain not in discovered_domains and global_mode:\n",
    "                discovered_domains.add(current_domain)\n",
    "                print(f\"NEW DOMAIN DISCOVERED: {current_domain}\")\n",
    "\n",
    "            # Extract links\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            new_links = 0\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                next_url = urljoin(current_url, link['href'])\n",
    "\n",
    "                if (is_valid_url(next_url, allowed_domains) and\n",
    "                    next_url not in visited and\n",
    "                    next_url not in [item[0] for item in queue]):\n",
    "\n",
    "                    queue.append((next_url, depth + 1))\n",
    "                    new_links += 1\n",
    "\n",
    "            print(f\"Indexed | +{new_links} new links queued\")\n",
    "\n",
    "            time.sleep(delay)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    print(f\"\\nCrawling complete! Indexed {len(visited)} pages.\")\n",
    "    if global_mode:\n",
    "        print(f\"Discovered {len(discovered_domains)} different domains.\")\n",
    "    print(\"Your personal search engine now knows more of the web!\")\n",
    "\n",
    "\n",
    "# ==================== USAGE EXAMPLES ====================\n",
    "\n",
    "engine = SearchEngine()  # Make sure your SearchEngine class is defined\n",
    "\n",
    "# 1. Single site deep crawl (original behavior)\n",
    "print(\"Example 1: Deep crawl of one site\")\n",
    "auto_crawl_and_index(\n",
    "    engine,\n",
    "    seed_urls=\"https://www.paulgraham.com/articles.html\",\n",
    "    max_pages=40,\n",
    "    max_depth=4,\n",
    "    delay=1.5,\n",
    "    global_mode=False\n",
    ")\n",
    "\n",
    "# 2. Global web exploration mode - discovers new sites!\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 2: GLOBAL MODE - Exploring the open web\")\n",
    "auto_crawl_and_index(\n",
    "    engine,\n",
    "    seed_urls=[\n",
    "        \"https://news.ycombinator.com\",\n",
    "        \"https://www.lesswrong.com\",\n",
    "        \"https://www.paulgraham.com\"\n",
    "    ],\n",
    "    max_pages=80,\n",
    "    max_depth=5,\n",
    "    delay=2.0,          # Be extra polite in global mode\n",
    "    global_mode=True   # This is the key!\n",
    ")\n",
    "\n",
    "# Test your growing search engine\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "results = engine.search(\"artificial intelligence ethics\")\n",
    "for i, (url, score) in enumerate(results[:5], 1):\n",
    "    print(f\"{i}. [{score:.4f}] {url}\")\n",
    "\n",
    "results = engine.search(\"best programming books\")\n",
    "for i, (url, score) in enumerate(results[:5], 1):\n",
    "    print(f\"{i}. [{score:.4f}] {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6755a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL MODE: Exploring freely across domains\n",
      "Initial seeds: 3 URLs\n",
      "Max pages: 50 | Max depth: 5 | Delay: 2.0s\n",
      "\n",
      "[1/50] Crawling: https://news.ycombinator.com (depth: 0)\n",
      "Failed to parse sitemap https://news.ycombinator.com/sitemap.xml: 404 Client Error: Not Found for url: https://news.ycombinator.com/sitemap.xml\n",
      "Indexed | +197 links queued\n",
      "[2/50] Crawling: https://www.reddit.com (depth: 0)\n",
      "Failed to parse sitemap https://www.reddit.com/sitemap.xml: not well-formed (invalid token): line 40, column 231\n",
      "Blocked by robots.txt: https://www.reddit.com\n",
      "[3/50] Crawling: https://en.wikipedia.org/wiki/Main_Page (depth: 0)\n",
      "Failed to parse sitemap https://en.wikipedia.org/sitemap.xml: 403 Client Error: Forbidden for url: https://en.wikipedia.org/sitemap.xml\n",
      "Blocked by robots.txt: https://en.wikipedia.org/wiki/Main_Page\n",
      "[4/50] Crawling: https://news.ycombinator.com/news (depth: 1)\n",
      "Indexed | +1 links queued\n",
      "[5/50] Crawling: https://news.ycombinator.com/newest (depth: 1)\n",
      "Indexed | +194 links queued\n",
      "[6/50] Crawling: https://news.ycombinator.com/front (depth: 1)\n",
      "Indexed | +96 links queued\n",
      "[7/50] Crawling: https://news.ycombinator.com/newcomments (depth: 1)\n",
      "Indexed | +151 links queued\n",
      "[8/50] Crawling: https://news.ycombinator.com/ask (depth: 1)\n",
      "Indexed | +92 links queued\n",
      "[9/50] Crawling: https://news.ycombinator.com/show (depth: 1)\n",
      "Indexed | +135 links queued\n",
      "[10/50] Crawling: https://news.ycombinator.com/jobs (depth: 1)\n",
      "Indexed | +68 links queued\n",
      "[11/50] Crawling: https://news.ycombinator.com/submit (depth: 1)\n",
      "[12/50] Crawling: https://news.ycombinator.com/login?goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/login?goto=news\n",
      "[13/50] Crawling: https://news.ycombinator.com/vote?id=46341305&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46341305&how=up&goto=news\n",
      "[14/50] Crawling: https://haveibeenflocked.com/news/cyble-downtime (depth: 1)\n",
      "Found sitemap in robots.txt: https://haveibeenflocked.com/sitemap.xml\n",
      "Added 5630 URLs from sitemap\n",
      "[15/50] Crawling: https://news.ycombinator.com/from?site=haveibeenflocked.com (depth: 1)\n",
      "Indexed | +8 links queued\n",
      "[16/50] Crawling: https://news.ycombinator.com/user?id=_a9 (depth: 1)\n",
      "Indexed | +5 links queued\n",
      "[17/50] Crawling: https://news.ycombinator.com/item?id=46341305 (depth: 1)\n",
      "Indexed | +351 links queued\n",
      "[18/50] Crawling: https://news.ycombinator.com/hide?id=46341305&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46341305&goto=news\n",
      "[19/50] Crawling: https://news.ycombinator.com/vote?id=46339600&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46339600&how=up&goto=news\n",
      "[20/50] Crawling: https://www.jmail.world (depth: 1)\n",
      "Found sitemap in robots.txt: https://jmail.world/sitemap.xml\n",
      "Added 3567 URLs from sitemap\n",
      "Indexed | +34 links queued\n",
      "New site discovered: https://www.jmail.world/\n",
      "[21/50] Crawling: https://news.ycombinator.com/from?site=jmail.world (depth: 1)\n",
      "Indexed | +20 links queued\n",
      "[22/50] Crawling: https://news.ycombinator.com/user?id=lukeigel (depth: 1)\n",
      "Indexed | +5 links queued\n",
      "[23/50] Crawling: https://news.ycombinator.com/item?id=46339600 (depth: 1)\n",
      "Indexed | +642 links queued\n",
      "[24/50] Crawling: https://news.ycombinator.com/hide?id=46339600&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46339600&goto=news\n",
      "[25/50] Crawling: https://news.ycombinator.com/vote?id=46342166&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46342166&how=up&goto=news\n",
      "[26/50] Crawling: https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/ (depth: 1)\n",
      "Added 113 URLs from default sitemap\n",
      "Indexed | +24 links queued\n",
      "New site discovered: https://metr.org/\n",
      "[27/50] Crawling: https://news.ycombinator.com/from?site=metr.org (depth: 1)\n",
      "Indexed | +81 links queued\n",
      "[28/50] Crawling: https://news.ycombinator.com/user?id=spicypete (depth: 1)\n",
      "Indexed | +5 links queued\n",
      "[29/50] Crawling: https://news.ycombinator.com/item?id=46342166 (depth: 1)\n",
      "Indexed | +369 links queued\n",
      "[30/50] Crawling: https://news.ycombinator.com/hide?id=46342166&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46342166&goto=news\n",
      "[31/50] Crawling: https://news.ycombinator.com/vote?id=46338339&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46338339&how=up&goto=news\n",
      "[32/50] Crawling: https://annas-archive.li/blog/backing-up-spotify.html (depth: 1)\n",
      "Failed to parse sitemap https://annas-archive.li/sitemap.xml: 404 Client Error: Not Found for url: https://annas-archive.li/sitemap.xml\n",
      "Indexed | +13 links queued\n",
      "New site discovered: https://annas-archive.li/\n",
      "[33/50] Crawling: https://news.ycombinator.com/from?site=annas-archive.li (depth: 1)\n",
      "Indexed | +13 links queued\n",
      "[34/50] Crawling: https://news.ycombinator.com/user?id=vitplister (depth: 1)\n",
      "Indexed | +5 links queued\n",
      "[35/50] Crawling: https://news.ycombinator.com/item?id=46338339 (depth: 1)\n",
      "Indexed | +1756 links queued\n",
      "[36/50] Crawling: https://news.ycombinator.com/hide?id=46338339&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46338339&goto=news\n",
      "[37/50] Crawling: https://news.ycombinator.com/vote?id=46342447&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46342447&how=up&goto=news\n",
      "[38/50] Crawling: https://www.ucsf.edu/news/2025/12/431206/indoor-tanning-makes-youthful-skin-much-older-genetic-level (depth: 1)\n",
      "Found sitemap in robots.txt: https://www.ucsf.edu/sitemap.xml\n",
      "Added 16484 URLs from sitemap\n",
      "Indexed | +64 links queued\n",
      "New site discovered: https://www.ucsf.edu/\n",
      "[39/50] Crawling: https://news.ycombinator.com/from?site=ucsf.edu (depth: 1)\n",
      "Indexed | +111 links queued\n",
      "[40/50] Crawling: https://news.ycombinator.com/user?id=SanjayMehta (depth: 1)\n",
      "Indexed | +5 links queued\n",
      "[41/50] Crawling: https://news.ycombinator.com/item?id=46342447 (depth: 1)\n",
      "Indexed | +41 links queued\n",
      "[42/50] Crawling: https://news.ycombinator.com/hide?id=46342447&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46342447&goto=news\n",
      "[43/50] Crawling: https://news.ycombinator.com/vote?id=46342859&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46342859&how=up&goto=news\n",
      "[44/50] Crawling: https://www.ruby-lang.org/en/ (depth: 1)\n",
      "Failed to parse sitemap https://www.ruby-lang.org/sitemap.xml: 404 Client Error: Not Found for url: https://www.ruby-lang.org/sitemap.xml\n",
      "Blocked by robots.txt: https://www.ruby-lang.org/en/\n",
      "[45/50] Crawling: https://news.ycombinator.com/from?site=ruby-lang.org (depth: 1)\n",
      "Indexed | +131 links queued\n",
      "[46/50] Crawling: https://news.ycombinator.com/user?id=psxuaw (depth: 1)\n",
      "Indexed | +5 links queued\n",
      "[47/50] Crawling: https://news.ycombinator.com/item?id=46342859 (depth: 1)\n",
      "Indexed | +65 links queued\n",
      "[48/50] Crawling: https://news.ycombinator.com/hide?id=46342859&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/hide?id=46342859&goto=news\n",
      "[49/50] Crawling: https://news.ycombinator.com/vote?id=46342528&how=up&goto=news (depth: 1)\n",
      "Blocked by robots.txt: https://news.ycombinator.com/vote?id=46342528&how=up&goto=news\n",
      "[50/50] Crawling: https://lareviewofbooks.org/article/isengard-in-oxford/ (depth: 1)\n",
      "Failed to parse sitemap https://lareviewofbooks.org/sitemap.xml: 404 Client Error: Not Found for url: https://lareviewofbooks.org/sitemap.xml\n",
      "Indexed | +49 links queued\n",
      "New site discovered: https://lareviewofbooks.org/\n",
      "\n",
      "Crawling complete! Indexed 50 pages.\n",
      "Discovered 5 new homepages for future seeds.\n",
      "\n",
      "Saved new seeds to new_seeds.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "from typing import Set, List\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "import xml.etree.ElementTree as ET  # For sitemap XML parsing\n",
    "\n",
    "\n",
    "def is_valid_url(url: str, allowed_domains: Set[str] | None = None) -> bool:\n",
    "    \"\"\"\n",
    "    Validate URL: proper scheme, not a file type, and optionally restricted to domains.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.scheme not in ('http', 'https'):\n",
    "        return False\n",
    "    if parsed.path.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.js', '.css', '.ico')):\n",
    "        return False\n",
    "    if allowed_domains is not None and parsed.netloc not in allowed_domains:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_content(html: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract clean readable text from HTML.\n",
    "    Removes noise and skips pages with insufficient content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\", \n",
    "                     \"iframe\", \"noscript\", \"form\", \"button\", \"input\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    if len(text.split()) < 30:\n",
    "        return None\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def get_robots_parser(domain: str, user_agent: str) -> RobotFileParser:\n",
    "    \"\"\"\n",
    "    Fetch and parse robots.txt for a domain.\n",
    "    \"\"\"\n",
    "    rp = RobotFileParser()\n",
    "    robots_url = urllib.parse.urljoin(domain, '/robots.txt')\n",
    "    \n",
    "    try:\n",
    "        with urlopen(robots_url, timeout=10) as response:\n",
    "            rp.parse(response.read().decode('utf-8').splitlines())\n",
    "    except Exception:\n",
    "        pass  # If unavailable, assume allowed\n",
    "    \n",
    "    return rp\n",
    "\n",
    "\n",
    "def parse_sitemap(domain: str, sitemap_url: str | None = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse sitemap.xml to extract URLs.\n",
    "    If sitemap_url is None, try default '/sitemap.xml'.\n",
    "    Returns list of URLs from <loc> tags.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    if sitemap_url is None:\n",
    "        sitemap_url = urllib.parse.urljoin(domain, '/sitemap.xml')\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(sitemap_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        root = ET.fromstring(response.text)\n",
    "        ns = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        \n",
    "        for loc in root.findall('.//sitemap:loc', ns):\n",
    "            urls.append(loc.text.strip())\n",
    "        \n",
    "        # If it's a sitemap index, recursively parse sub-sitemaps (simple version, limit recursion)\n",
    "        if 'sitemapindex' in root.tag:\n",
    "            for sub_sitemap in root.findall('sitemap:sitemap/sitemap:loc', ns):\n",
    "                urls.extend(parse_sitemap(domain, sub_sitemap.text.strip()))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse sitemap {sitemap_url}: {e}\")\n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_sitemap_from_robots(rp: RobotFileParser) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract sitemap URL from robots.txt if present.\n",
    "    \"\"\"\n",
    "    sitemaps = rp.site_maps()\n",
    "    if sitemaps is not None:\n",
    "        for line in sitemaps:\n",
    "            if line:\n",
    "                return line\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_homepage_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert any URL to its homepage.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return urlunparse((parsed.scheme, parsed.netloc, '/', '', '', ''))\n",
    "\n",
    "\n",
    "def auto_crawl_and_index(\n",
    "    engine,\n",
    "    initial_seed_urls,\n",
    "    max_pages: int = 200,\n",
    "    max_depth: int = 6,\n",
    "    delay: float = 2.0,\n",
    "    global_mode: bool = True\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Global web crawler with sitemap parsing.\n",
    "    Discovers new sites and uses sitemaps to find more URLs within domains.\n",
    "    Returns list of newly discovered homepages for future seeds.\n",
    "    \"\"\"\n",
    "    if isinstance(initial_seed_urls, str):\n",
    "        initial_seed_urls = [initial_seed_urls]\n",
    "    \n",
    "    seed_urls = []\n",
    "    for url in initial_seed_urls:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url\n",
    "        seed_urls.append(url)\n",
    "    \n",
    "    if global_mode:\n",
    "        allowed_domains = None\n",
    "        print(\"GLOBAL MODE: Exploring freely across domains\")\n",
    "    else:\n",
    "        allowed_domains = {urlparse(u).netloc for u in seed_urls}\n",
    "        print(f\"DOMAIN-LOCKED MODE: Restricted to initial domains\")\n",
    "    \n",
    "    print(f\"Initial seeds: {len(seed_urls)} URLs\")\n",
    "    print(f\"Max pages: {max_pages} | Max depth: {max_depth} | Delay: {delay}s\\n\")\n",
    "\n",
    "    visited = set()\n",
    "    queue = deque([(url, 0) for url in seed_urls])\n",
    "    discovered_homepages = set()\n",
    "    robots_parsers = {}\n",
    "    user_agent = 'MyGlobalSearchBot/1.0 (+https://yourwebsite.com/bot)'\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        current_url, depth = queue.popleft()\n",
    "        \n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        print(f\"[{len(visited)+1}/{max_pages}] Crawling: {current_url} (depth: {depth})\")\n",
    "\n",
    "        parsed_url = urlparse(current_url)\n",
    "        domain = f\"{parsed_url.scheme}://{parsed_url.netloc}/\"\n",
    "        \n",
    "        if domain not in robots_parsers:\n",
    "            rp = get_robots_parser(domain, user_agent)\n",
    "            robots_parsers[domain] = rp\n",
    "            \n",
    "            # Check for sitemap in robots.txt and parse it\n",
    "            sitemap_url = get_sitemap_from_robots(rp)\n",
    "            if sitemap_url:\n",
    "                print(f\"Found sitemap in robots.txt: {sitemap_url}\")\n",
    "                sitemap_urls = parse_sitemap(domain, sitemap_url)\n",
    "                for new_url in sitemap_urls:\n",
    "                    if is_valid_url(new_url, allowed_domains) and new_url not in visited:\n",
    "                        queue.append((new_url, depth + 1))\n",
    "                print(f\"Added {len(sitemap_urls)} URLs from sitemap\")\n",
    "            else:\n",
    "                # Fallback: try default sitemap.xml\n",
    "                default_sitemap_urls = parse_sitemap(domain)\n",
    "                for new_url in default_sitemap_urls:\n",
    "                    if is_valid_url(new_url, allowed_domains) and new_url not in visited:\n",
    "                        queue.append((new_url, depth + 1))\n",
    "                if default_sitemap_urls:\n",
    "                    print(f\"Added {len(default_sitemap_urls)} URLs from default sitemap\")\n",
    "\n",
    "        rp = robots_parsers[domain]\n",
    "        if not rp.can_fetch(user_agent, current_url):\n",
    "            print(f\"Blocked by robots.txt: {current_url}\")\n",
    "            visited.add(current_url)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            headers = {'User-Agent': user_agent}\n",
    "            response = requests.get(current_url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            content = clean_content(response.text)\n",
    "            if content is None:\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            engine.index(current_url, content)\n",
    "            visited.add(current_url)\n",
    "\n",
    "            current_homepage = get_homepage_url(current_url)\n",
    "            if current_homepage not in [get_homepage_url(s) for s in seed_urls]:\n",
    "                discovered_homepages.add(current_homepage)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            new_links = 0\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                next_url = urljoin(current_url, link['href'])\n",
    "\n",
    "                if (is_valid_url(next_url, allowed_domains) and\n",
    "                    next_url not in visited and\n",
    "                    next_url not in [item[0] for item in queue]):\n",
    "\n",
    "                    queue.append((next_url, depth + 1))\n",
    "                    new_links += 1\n",
    "\n",
    "            print(f\"Indexed | +{new_links} links queued\")\n",
    "            if current_homepage in discovered_homepages:\n",
    "                print(f\"New site discovered: {current_homepage}\")\n",
    "\n",
    "            time.sleep(delay)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    print(f\"\\nCrawling complete! Indexed {len(visited)} pages.\")\n",
    "    print(f\"Discovered {len(discovered_homepages)} new homepages for future seeds.\")\n",
    "    \n",
    "    return list(discovered_homepages)\n",
    "\n",
    "\n",
    "# ==================== USAGE ====================\n",
    "\n",
    "engine = SearchEngine()\n",
    "\n",
    "initial_seeds = [\n",
    "    \"https://news.ycombinator.com\",\n",
    "    \"https://www.reddit.com\",\n",
    "    \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "]\n",
    "\n",
    "new_seeds = auto_crawl_and_index(\n",
    "    engine,\n",
    "    initial_seed_urls=initial_seeds,\n",
    "    max_pages=50,\n",
    "    max_depth=5,\n",
    "    delay=2.0,\n",
    "    global_mode=True\n",
    ")\n",
    "\n",
    "# Save new seeds\n",
    "with open(\"new_seeds.txt\", \"w\") as f:\n",
    "    for seed in new_seeds:\n",
    "        f.write(seed + \"\\n\")\n",
    "\n",
    "print(\"\\nSaved new seeds to new_seeds.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_engine (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
