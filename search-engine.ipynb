{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0b1170",
   "metadata": {},
   "source": [
    "### Ranker\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\text{IDF}(q_i) \\frac{f(q_i, D) \\times (k_1 + 1)}{f(q_i, D) + k_1 (1 - b + b\\frac{|D|}{\\text{avgdl}})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q$: query  \n",
    "- $D$: $|D|$ is document length  \n",
    "- $\\text{avgdl}$: average length of a document  \n",
    "- $k_1$, $b$: free parameters  \n",
    "- $f(q_i, D)$: the number of times that keyword $q_i$ appears in the document $D$  \n",
    "- $\\text{IDF}(q_i)$: the inverse document frequency  \n",
    "\n",
    "$$\n",
    "\\text{IDF}(q_i) = \\ln(1 + \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$: the number of documents  \n",
    "- $n(q_i)$: the number of documents containing $q_i$\n",
    "\n",
    "[reference](https://www.alexmolas.com/2024/02/05/a-search-engine-in-80-lines.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a6513ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import string\n",
    "from typing import TypeAlias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70d1e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlScores: TypeAlias = dict[str, float]\n",
    "\n",
    "def normalize_string(input_string: str) -> str:\n",
    "    \"\"\"구두점 제거, 소문자 변환, 중복 공백 정리\"\"\"\n",
    "    translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    string_without_punc = input_string.translate(translation_table)\n",
    "    return ' '.join(string_without_punc.split()).lower()\n",
    "\n",
    "\n",
    "def update_url_scores(old: UrlScores, new: UrlScores) -> UrlScores:\n",
    "    \"\"\"새 점수를 기존 딕셔너리에 합산\"\"\"\n",
    "    for url, score in new.items():\n",
    "        old[url] = old.get(url, 0.0) + score\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e81235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        # 역인덱스: 단어 → {URL: 빈도수}\n",
    "        self._index: defaultdict[str, defaultdict[str, int]] = defaultdict(lambda: defaultdict(int))\n",
    "        # 문서 저장: URL → 내용 (길이 계산용)\n",
    "        self._documents: dict[str, str] = {}\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "    @property\n",
    "    def number_of_documents(self) -> int:\n",
    "        return len(self._documents)\n",
    "\n",
    "    @property\n",
    "    def avdl(self) -> float:\n",
    "        \"\"\"전체 문서의 평균 단어 길이\"\"\"\n",
    "        if not self._documents:\n",
    "            return 0.0\n",
    "        total_words = sum(len(content.split()) for content in self._documents.values())\n",
    "        return total_words / self.number_of_documents\n",
    "\n",
    "    def idf(self, kw: str) -> float:\n",
    "        \"\"\"Inverse Document Frequency\"\"\"\n",
    "        N = self.number_of_documents\n",
    "        kw_norm = normalize_string(kw)\n",
    "        n_kw = len(self._index[kw_norm])\n",
    "        return log((N - n_kw + 0.5) / (n_kw + 0.5) + 1)\n",
    "\n",
    "    def bm25(self, kw: str) -> UrlScores:\n",
    "        \"\"\"하나의 키워드에 대한 BM25 점수 계산\"\"\"\n",
    "        kw_norm = normalize_string(kw)\n",
    "        if kw_norm not in self._index:\n",
    "            return {}\n",
    "\n",
    "        result: UrlScores = {}\n",
    "        idf_score = self.idf(kw)\n",
    "        avdl = self.avdl\n",
    "\n",
    "        for url, freq in self._index[kw_norm].items():\n",
    "            doc_length = len(self._documents[url].split())\n",
    "            numerator = freq * (self.k1 + 1)\n",
    "            denominator = freq + self.k1 * (1 - self.b + self.b * doc_length / avdl)\n",
    "            result[url] = idf_score * (numerator / denominator)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def search(self, query: str, top_n: int = 5) -> list[tuple[str, float]]:\n",
    "        \"\"\"쿼리 검색 → 상위 top_n 결과 반환 (점수 내림차순)\"\"\"\n",
    "        keywords = normalize_string(query).split()\n",
    "        url_scores: UrlScores = {}\n",
    "\n",
    "        for kw in keywords:\n",
    "            kw_scores = self.bm25(kw)\n",
    "            update_url_scores(url_scores, kw_scores)\n",
    "\n",
    "        ranked = sorted(url_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_n]\n",
    "\n",
    "    def index(self, url: str, content: str) -> None:\n",
    "        \"\"\"단일 문서 인덱싱\"\"\"\n",
    "        self._documents[url] = content\n",
    "        words = normalize_string(content).split()\n",
    "        for word in words:\n",
    "            self._index[word][url] += 1\n",
    "\n",
    "    def bulk_index(self, documents: list[tuple[str, str]]) -> None:\n",
    "        \"\"\"여러 문서 일괄 인덱싱\"\"\"\n",
    "        for url, content in documents:\n",
    "            self.index(url, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14863015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱싱된 문서: 4개\n",
      "평균 문서 길이: 10.25 단어\n",
      "\n",
      "1. https://example.com/post1 (점수: 1.3126)\n",
      "2. https://example.com/post4 (점수: 0.1229)\n",
      "3. https://example.com/post2 (점수: 0.1115)\n",
      "4. https://example.com/post3 (점수: 0.0905)\n"
     ]
    }
   ],
   "source": [
    "# 검색 엔진 생성\n",
    "engine = SearchEngine()\n",
    "\n",
    "# 샘플 문서\n",
    "sample_docs = [\n",
    "    (\"https://example.com/post1\", \"Python is a great programming language. Python is easy to learn.\"),\n",
    "    (\"https://example.com/post2\", \"Machine learning with Python is very popular these days.\"),\n",
    "    (\"https://example.com/post3\", \"JavaScript is used for web development. Python can also be used on the web.\"),\n",
    "    (\"https://example.com/post4\", \"I love coding in Python every day.\"),\n",
    "]\n",
    "\n",
    "engine.bulk_index(sample_docs)\n",
    "\n",
    "print(f\"인덱싱된 문서: {engine.number_of_documents}개\")\n",
    "print(f\"평균 문서 길이: {engine.avdl:.2f} 단어\\n\")\n",
    "\n",
    "# 검색 테스트\n",
    "results = engine.search(\"Python programming\", top_n=4)\n",
    "for rank, (url, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. {url} (점수: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfb1c7",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "[wikidocs](https://wikidocs.net/31698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "646615de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c119837",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = {\n",
    "    'a', 'an', 'and', 'the', 'or', 'of', 'to', 'in', 'for', 'on', 'with', 'at', 'by',\n",
    "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
    "    'my', 'your', 'his', 'its', 'our', 'their', 'this', 'that', 'these', 'those',\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do',\n",
    "    'does', 'did', 'will', 'would', 'shall', 'should', 'may', 'might', 'must',\n",
    "    'can', 'could', 'as', 'but', 'if', 'because', 'until', 'while', 'about',\n",
    "    'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "    'below', 'from', 'up', 'down', 'out', 'off', 'over', 'under', 'again', 'further',\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "    'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',\n",
    "    'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very'\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in STOP_WORDS and token.strip() != '']\n",
    "    tokens = [stem_token(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def stem_token(token):\n",
    "    suffixes = ['ing', 'ly', 'ed', 'es', 's']\n",
    "    for suffix in suffixes:\n",
    "        if token.endswith(suffix) and len(token) > len(suffix) + 1:\n",
    "            token = token[:-len(suffix)]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8b53c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(documents, index_file='inverted_index.csv'):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        tokens = preprocess_text(doc)\n",
    "        term_positions = defaultdict(list)\n",
    "        for pos, term in enumerate(tokens):\n",
    "            term_positions[term].append(pos)\n",
    "        for term, positions in term_positions.items():\n",
    "            inverted_index[term].append((doc_id, positions))\n",
    "    \n",
    "    # CSV 저장\n",
    "    with open(index_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['term', 'doc_id', 'positions'])\n",
    "        for term, doc_info in inverted_index.items():\n",
    "            for doc_id, positions in doc_info:\n",
    "                writer.writerow([term, doc_id, str(positions)])\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39e7a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(documents, inverted_index):\n",
    "    num_docs = len(documents)\n",
    "    tfidf = defaultdict(dict)\n",
    "    doc_lengths = [len(preprocess_text(doc)) for doc in documents]\n",
    "    doc_freq = {term: len(entries) for term, entries in inverted_index.items()}\n",
    "    \n",
    "    for term, entries in inverted_index.items():\n",
    "        idf = math.log(num_docs / (doc_freq[term] + 1))\n",
    "        for doc_id, positions in entries:\n",
    "            tf = len(positions) / doc_lengths[doc_id] if doc_lengths[doc_id] > 0 else 0\n",
    "            tfidf[doc_id][term] = tf * idf\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85cca302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, documents, inverted_index, tfidf_scores, top_n=5):\n",
    "    query_terms = preprocess_text(query)\n",
    "    if not query_terms:\n",
    "        return []\n",
    "    \n",
    "    relevant_docs = set()\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            for doc_id, _ in inverted_index[term]:\n",
    "                relevant_docs.add(doc_id)\n",
    "    \n",
    "    scores = []\n",
    "    for doc_id in relevant_docs:\n",
    "        score = sum(tfidf_scores.get(doc_id, {}).get(term, 0) for term in query_terms)\n",
    "        score /= len(query_terms) or 1\n",
    "        scores.append((doc_id, score))\n",
    "    \n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    results = [(documents[doc_id], score) for doc_id, score in scores[:top_n] if score > 0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81d170c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Score: 0.0422 - Building a search engine involves crawling, indexing, and querying large datasets.\n",
      "2. Score: 0.0339 - Python is a versatile programming language used for web development and data analysis.\n",
      "3. Score: 0.0093 - Bing integrates AI to improve search accuracy and user experience.\n",
      "4. Score: 0.0068 - Search engines like Google use complex ranking systems to deliver relevant results.\n"
     ]
    }
   ],
   "source": [
    "# 샘플 문서 (이전처럼 위키나 블로그에서 가져온 텍스트로 대체 가능)\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data analysis.\",\n",
    "    \"Machine learning algorithms can predict outcomes based on historical data.\",\n",
    "    \"Search engines like Google use complex ranking systems to deliver relevant results.\",\n",
    "    \"Bing integrates AI to improve search accuracy and user experience.\",\n",
    "    \"Building a search engine involves crawling, indexing, and querying large datasets.\"\n",
    "]\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "tfidf_scores = calculate_tfidf(documents, inverted_index)\n",
    "\n",
    "# 검색 테스트\n",
    "results = search(\"search engine python\", documents, inverted_index, tfidf_scores)\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"{i}. Score: {score:.4f} - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30fcd4",
   "metadata": {},
   "source": [
    "### Scrap sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "806241de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting automatic crawl from: https://www.paulgraham.com/articles.html\n",
      "Domain restricted to: www.paulgraham.com\n",
      "Max pages: 30, Max depth: 3\n",
      "\n",
      "[1/30] Crawling: https://www.paulgraham.com/articles.html (depth: 0)\n",
      "> Indexed successfully | Found 230 new links\n",
      "[2/30] Crawling: https://www.paulgraham.com/index.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[3/30] Crawling: https://www.paulgraham.com/greatwork.html (depth: 1)\n",
      "> Indexed successfully | Found 29 new links\n",
      "[4/30] Crawling: https://www.paulgraham.com/kids.html (depth: 1)\n",
      "> Indexed successfully | Found 1 new links\n",
      "[5/30] Crawling: https://www.paulgraham.com/selfindulgence.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[6/30] Crawling: https://www.paulgraham.com/field.html (depth: 1)\n",
      "> Indexed successfully | Found 1 new links\n",
      "[7/30] Crawling: https://www.paulgraham.com/goodwriting.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[8/30] Crawling: https://www.paulgraham.com/do.html (depth: 1)\n",
      "> Indexed successfully | Found 2 new links\n",
      "[9/30] Crawling: https://www.paulgraham.com/woke.html (depth: 1)\n",
      "> Indexed successfully | Found 19 new links\n",
      "[10/30] Crawling: https://www.paulgraham.com/writes.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[11/30] Crawling: https://www.paulgraham.com/when.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[12/30] Crawling: https://www.paulgraham.com/foundermode.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[13/30] Crawling: https://www.paulgraham.com/persistence.html (depth: 1)\n",
      "> Indexed successfully | Found 6 new links\n",
      "[14/30] Crawling: https://www.paulgraham.com/reddits.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[15/30] Crawling: https://www.paulgraham.com/google.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[16/30] Crawling: https://www.paulgraham.com/best.html (depth: 1)\n",
      "> Indexed successfully | Found 12 new links\n",
      "[17/30] Crawling: https://www.paulgraham.com/superlinear.html (depth: 1)\n",
      "> Indexed successfully | Found 12 new links\n",
      "[18/30] Crawling: https://www.paulgraham.com/getideas.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[19/30] Crawling: https://www.paulgraham.com/read.html (depth: 1)\n",
      "> Indexed successfully | Found 2 new links\n",
      "[20/30] Crawling: https://www.paulgraham.com/want.html (depth: 1)\n",
      "> Indexed successfully | Found 2 new links\n",
      "[21/30] Crawling: https://www.paulgraham.com/alien.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[22/30] Crawling: https://www.paulgraham.com/users.html (depth: 1)\n",
      "> Indexed successfully | Found 5 new links\n",
      "[23/30] Crawling: https://www.paulgraham.com/heresy.html (depth: 1)\n",
      "> Indexed successfully | Found 6 new links\n",
      "[24/30] Crawling: https://www.paulgraham.com/words.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "[25/30] Crawling: https://www.paulgraham.com/goodtaste.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[26/30] Crawling: https://www.paulgraham.com/smart.html (depth: 1)\n",
      "> Indexed successfully | Found 4 new links\n",
      "[27/30] Crawling: https://www.paulgraham.com/weird.html (depth: 1)\n",
      "> Indexed successfully | Found 0 new links\n",
      "[28/30] Crawling: https://www.paulgraham.com/hwh.html (depth: 1)\n",
      "> Indexed successfully | Found 9 new links\n",
      "[29/30] Crawling: https://www.paulgraham.com/own.html (depth: 1)\n",
      "> Indexed successfully | Found 5 new links\n",
      "[30/30] Crawling: https://www.paulgraham.com/fn.html (depth: 1)\n",
      "> Indexed successfully | Found 3 new links\n",
      "\n",
      "Crawling finished! Total indexed pages: 30\n",
      "Your search engine is now ready to query these pages!\n",
      "\n",
      "============================================================\n",
      "1. [5.4732] https://www.paulgraham.com/articles.html\n",
      "2. [4.7139] https://www.paulgraham.com/users.html\n",
      "3. [4.0784] https://www.paulgraham.com/reddits.html\n",
      "4. [2.0999] https://www.paulgraham.com/google.html\n",
      "5. [1.8857] https://www.paulgraham.com/superlinear.html\n",
      "\n",
      "\n",
      "1. [7.5700] https://www.paulgraham.com/articles.html\n",
      "2. [6.8176] https://www.paulgraham.com/weird.html\n",
      "3. [4.0169] https://www.paulgraham.com/google.html\n",
      "4. [3.9901] https://www.paulgraham.com/own.html\n",
      "5. [3.6844] https://www.paulgraham.com/reddits.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def is_valid_url(url: str, base_domain: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a URL is valid and belongs to the same domain as the start URL.\n",
    "    Also skips non-HTML files like images, PDFs, etc.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return (\n",
    "        parsed.scheme in ('http', 'https') and\n",
    "        parsed.netloc == base_domain and\n",
    "        not parsed.path.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.js', '.css'))\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_content(html: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract clean, readable text from HTML.\n",
    "    Removes scripts, styles, navigation, footers, etc., and skips pages with too little content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements that usually don't contain useful content\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\", \"iframe\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Skip pages with very little text (e.g., login pages, error pages)\n",
    "    if len(text.split()) < 20:\n",
    "        return None\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def auto_crawl_and_index(\n",
    "    engine,\n",
    "    start_url: str,\n",
    "    max_pages: int = 50,\n",
    "    max_depth: int = 5,\n",
    "    delay: float = 1.0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fully automatic web crawler with indexing.\n",
    "    Starts from a seed URL and follows links breadth-first (BFS), just like Google.\n",
    "    Only crawls within the same domain and respects basic politeness rules.\n",
    "    \"\"\"\n",
    "    # Ensure URL has protocol\n",
    "    if not start_url.startswith(('http://', 'https://')):\n",
    "        start_url = 'https://' + start_url\n",
    "    \n",
    "    base_domain = urlparse(start_url).netloc\n",
    "    visited = set()\n",
    "    queue = deque([(start_url, 0)])  # (url, current_depth)\n",
    "\n",
    "    print(f\"Starting automatic crawl from: {start_url}\")\n",
    "    print(f\"Domain restricted to: {base_domain}\")\n",
    "    print(f\"Max pages: {max_pages}, Max depth: {max_depth}\\n\")\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        current_url, depth = queue.popleft()\n",
    "\n",
    "        # Skip if already visited or too deep\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        print(f\"[{len(visited)+1}/{max_pages}] Crawling: {current_url} (depth: {depth})\")\n",
    "\n",
    "        try:\n",
    "            # Polite User-Agent (important to avoid being blocked)\n",
    "            headers = {\n",
    "                'User-Agent': 'MyPersonalSearchBot/1.0 (+https://yourwebsite.com/bot-info)'\n",
    "            }\n",
    "            response = requests.get(current_url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Skip non-HTML content\n",
    "            content_type = response.headers.get('Content-Type', '')\n",
    "            if 'text/html' not in content_type:\n",
    "                print(f\"Not HTML, skipping\")\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            content = clean_content(response.text)\n",
    "            if content is None:\n",
    "                print(f\"  → Too little content, skipping\")\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            # Index the page into your SearchEngine\n",
    "            engine.index(current_url, content)\n",
    "            visited.add(current_url)\n",
    "\n",
    "            # Extract and enqueue new links\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            new_links = 0\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                next_url = urljoin(current_url, link['href'])\n",
    "\n",
    "                # Add only valid, unvisited internal links\n",
    "                if (is_valid_url(next_url, base_domain) and\n",
    "                    next_url not in visited and\n",
    "                    next_url not in [item[0] for item in queue]):\n",
    "                    \n",
    "                    queue.append((next_url, depth + 1))\n",
    "                    new_links += 1\n",
    "\n",
    "            print(f\"> Indexed successfully | Found {new_links} new links\")\n",
    "\n",
    "            # Be respectful: delay between requests\n",
    "            time.sleep(delay)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    print(f\"\\nCrawling finished! Total indexed pages: {len(visited)}\")\n",
    "    print(\"Your search engine is now ready to query these pages!\")\n",
    "\n",
    "\n",
    "# ==================== Example Usage ====================\n",
    "\n",
    "# Create your SearchEngine instance (make sure the class is defined earlier)\n",
    "engine = SearchEngine()\n",
    "\n",
    "# Start crawling from a seed page\n",
    "# Great test sites: small blogs, essay collections, documentation sites\n",
    "auto_crawl_and_index(\n",
    "    engine,\n",
    "    start_url=\"https://www.paulgraham.com/articles.html\",  # Paul Graham's essays (excellent content)\n",
    "    max_pages=30,\n",
    "    max_depth=3,\n",
    "    delay=1.5  # Be polite – 1.5 seconds between requests\n",
    ")\n",
    "\n",
    "# Test searches\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "results = engine.search(\"startup funding\", top_n=5)\n",
    "for rank, (url, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. [{score:.4f}] {url}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "results = engine.search(\"programming language design\")\n",
    "for rank, (url, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. [{score:.4f}] {url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_engine (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
