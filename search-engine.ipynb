{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0b1170",
   "metadata": {},
   "source": [
    "### Ranker\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\text{IDF}(q_i) \\frac{f(q_i, D) \\times (k_1 + 1)}{f(q_i, D) + k_1 (1 - b + b\\frac{|D|}{\\text{avgdl}})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q$: query  \n",
    "- $D$: $|D|$ is document length  \n",
    "- $\\text{avgdl}$: average length of a document  \n",
    "- $k_1$, $b$: free parameters  \n",
    "- $f(q_i, D)$: the number of times that keyword $q_i$ appears in the document $D$  \n",
    "- $\\text{IDF}(q_i)$: the inverse document frequency  \n",
    "\n",
    "$$\n",
    "\\text{IDF}(q_i) = \\ln(1 + \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$: the number of documents  \n",
    "- $n(q_i)$: the number of documents containing $q_i$\n",
    "\n",
    "[reference](https://www.alexmolas.com/2024/02/05/a-search-engine-in-80-lines.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6513ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import string\n",
    "from typing import TypeAlias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "UrlScores: TypeAlias = dict[str, float]\n",
    "\n",
    "def normalize_string(input_string: str) -> str:\n",
    "    \"\"\"구두점 제거, 소문자 변환, 중복 공백 정리\"\"\"\n",
    "    translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    string_without_punc = input_string.translate(translation_table)\n",
    "    return ' '.join(string_without_punc.split()).lower()\n",
    "\n",
    "\n",
    "def update_url_scores(old: UrlScores, new: UrlScores) -> UrlScores:\n",
    "    \"\"\"새 점수를 기존 딕셔너리에 합산\"\"\"\n",
    "    for url, score in new.items():\n",
    "        old[url] = old.get(url, 0.0) + score\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        # 역인덱스: 단어 → {URL: 빈도수}\n",
    "        self._index: defaultdict[str, defaultdict[str, int]] = defaultdict(lambda: defaultdict(int))\n",
    "        # 문서 저장: URL → 내용 (길이 계산용)\n",
    "        self._documents: dict[str, str] = {}\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "    @property\n",
    "    def number_of_documents(self) -> int:\n",
    "        return len(self._documents)\n",
    "\n",
    "    @property\n",
    "    def avdl(self) -> float:\n",
    "        \"\"\"전체 문서의 평균 단어 길이\"\"\"\n",
    "        if not self._documents:\n",
    "            return 0.0\n",
    "        total_words = sum(len(content.split()) for content in self._documents.values())\n",
    "        return total_words / self.number_of_documents\n",
    "\n",
    "    def idf(self, kw: str) -> float:\n",
    "        \"\"\"Inverse Document Frequency\"\"\"\n",
    "        N = self.number_of_documents\n",
    "        kw_norm = normalize_string(kw)\n",
    "        n_kw = len(self._index[kw_norm])\n",
    "        return log((N - n_kw + 0.5) / (n_kw + 0.5) + 1)\n",
    "\n",
    "    def bm25(self, kw: str) -> UrlScores:\n",
    "        \"\"\"하나의 키워드에 대한 BM25 점수 계산\"\"\"\n",
    "        kw_norm = normalize_string(kw)\n",
    "        if kw_norm not in self._index:\n",
    "            return {}\n",
    "\n",
    "        result: UrlScores = {}\n",
    "        idf_score = self.idf(kw)\n",
    "        avdl = self.avdl\n",
    "\n",
    "        for url, freq in self._index[kw_norm].items():\n",
    "            doc_length = len(self._documents[url].split())\n",
    "            numerator = freq * (self.k1 + 1)\n",
    "            denominator = freq + self.k1 * (1 - self.b + self.b * doc_length / avdl)\n",
    "            result[url] = idf_score * (numerator / denominator)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def search(self, query: str, top_n: int = 5) -> list[tuple[str, float]]:\n",
    "        \"\"\"쿼리 검색 → 상위 top_n 결과 반환 (점수 내림차순)\"\"\"\n",
    "        keywords = normalize_string(query).split()\n",
    "        url_scores: UrlScores = {}\n",
    "\n",
    "        for kw in keywords:\n",
    "            kw_scores = self.bm25(kw)\n",
    "            update_url_scores(url_scores, kw_scores)\n",
    "\n",
    "        ranked = sorted(url_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_n]\n",
    "\n",
    "    def index(self, url: str, content: str) -> None:\n",
    "        \"\"\"단일 문서 인덱싱\"\"\"\n",
    "        self._documents[url] = content\n",
    "        words = normalize_string(content).split()\n",
    "        for word in words:\n",
    "            self._index[word][url] += 1\n",
    "\n",
    "    def bulk_index(self, documents: list[tuple[str, str]]) -> None:\n",
    "        \"\"\"여러 문서 일괄 인덱싱\"\"\"\n",
    "        for url, content in documents:\n",
    "            self.index(url, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14863015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 엔진 생성\n",
    "engine = SearchEngine()\n",
    "\n",
    "# 샘플 문서\n",
    "sample_docs = [\n",
    "    (\"https://example.com/post1\", \"Python is a great programming language. Python is easy to learn.\"),\n",
    "    (\"https://example.com/post2\", \"Machine learning with Python is very popular these days.\"),\n",
    "    (\"https://example.com/post3\", \"JavaScript is used for web development. Python can also be used on the web.\"),\n",
    "    (\"https://example.com/post4\", \"I love coding in Python every day.\"),\n",
    "]\n",
    "\n",
    "engine.bulk_index(sample_docs)\n",
    "\n",
    "print(f\"인덱싱된 문서: {engine.number_of_documents}개\")\n",
    "print(f\"평균 문서 길이: {engine.avdl:.2f} 단어\\n\")\n",
    "\n",
    "# 검색 테스트\n",
    "results = engine.search(\"Python programming\", top_n=4)\n",
    "for rank, (url, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. {url} (점수: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfb1c7",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "[wikidocs](https://wikidocs.net/31698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646615de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c119837",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = {\n",
    "    'a', 'an', 'and', 'the', 'or', 'of', 'to', 'in', 'for', 'on', 'with', 'at', 'by',\n",
    "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
    "    'my', 'your', 'his', 'its', 'our', 'their', 'this', 'that', 'these', 'those',\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do',\n",
    "    'does', 'did', 'will', 'would', 'shall', 'should', 'may', 'might', 'must',\n",
    "    'can', 'could', 'as', 'but', 'if', 'because', 'until', 'while', 'about',\n",
    "    'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "    'below', 'from', 'up', 'down', 'out', 'off', 'over', 'under', 'again', 'further',\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "    'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',\n",
    "    'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very'\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in STOP_WORDS and token.strip() != '']\n",
    "    tokens = [stem_token(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def stem_token(token):\n",
    "    suffixes = ['ing', 'ly', 'ed', 'es', 's']\n",
    "    for suffix in suffixes:\n",
    "        if token.endswith(suffix) and len(token) > len(suffix) + 1:\n",
    "            token = token[:-len(suffix)]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b53c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(documents, index_file='inverted_index.csv'):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        tokens = preprocess_text(doc)\n",
    "        term_positions = defaultdict(list)\n",
    "        for pos, term in enumerate(tokens):\n",
    "            term_positions[term].append(pos)\n",
    "        for term, positions in term_positions.items():\n",
    "            inverted_index[term].append((doc_id, positions))\n",
    "    \n",
    "    # CSV 저장\n",
    "    with open(index_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['term', 'doc_id', 'positions'])\n",
    "        for term, doc_info in inverted_index.items():\n",
    "            for doc_id, positions in doc_info:\n",
    "                writer.writerow([term, doc_id, str(positions)])\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(documents, inverted_index):\n",
    "    num_docs = len(documents)\n",
    "    tfidf = defaultdict(dict)\n",
    "    doc_lengths = [len(preprocess_text(doc)) for doc in documents]\n",
    "    doc_freq = {term: len(entries) for term, entries in inverted_index.items()}\n",
    "    \n",
    "    for term, entries in inverted_index.items():\n",
    "        idf = math.log(num_docs / (doc_freq[term] + 1))\n",
    "        for doc_id, positions in entries:\n",
    "            tf = len(positions) / doc_lengths[doc_id] if doc_lengths[doc_id] > 0 else 0\n",
    "            tfidf[doc_id][term] = tf * idf\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cca302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, documents, inverted_index, tfidf_scores, top_n=5):\n",
    "    query_terms = preprocess_text(query)\n",
    "    if not query_terms:\n",
    "        return []\n",
    "    \n",
    "    relevant_docs = set()\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            for doc_id, _ in inverted_index[term]:\n",
    "                relevant_docs.add(doc_id)\n",
    "    \n",
    "    scores = []\n",
    "    for doc_id in relevant_docs:\n",
    "        score = sum(tfidf_scores.get(doc_id, {}).get(term, 0) for term in query_terms)\n",
    "        score /= len(query_terms) or 1\n",
    "        scores.append((doc_id, score))\n",
    "    \n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    results = [(documents[doc_id], score) for doc_id, score in scores[:top_n] if score > 0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d170c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 문서 (이전처럼 위키나 블로그에서 가져온 텍스트로 대체 가능)\n",
    "documents = [\n",
    "    \"Python is a versatile programming language used for web development and data analysis.\",\n",
    "    \"Machine learning algorithms can predict outcomes based on historical data.\",\n",
    "    \"Search engines like Google use complex ranking systems to deliver relevant results.\",\n",
    "    \"Bing integrates AI to improve search accuracy and user experience.\",\n",
    "    \"Building a search engine involves crawling, indexing, and querying large datasets.\"\n",
    "]\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "tfidf_scores = calculate_tfidf(documents, inverted_index)\n",
    "\n",
    "# 검색 테스트\n",
    "results = search(\"search engine python\", documents, inverted_index, tfidf_scores)\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"{i}. Score: {score:.4f} - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30fcd4",
   "metadata": {},
   "source": [
    "### Scrap sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806241de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def is_valid_url(url: str, base_domain: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a URL is valid and belongs to the same domain as the start URL.\n",
    "    Also skips non-HTML files like images, PDFs, etc.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return (\n",
    "        parsed.scheme in ('http', 'https') and\n",
    "        parsed.netloc == base_domain and\n",
    "        not parsed.path.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.js', '.css'))\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_content(html: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract clean, readable text from HTML.\n",
    "    Removes scripts, styles, navigation, footers, etc., and skips pages with too little content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements that usually don't contain useful content\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\", \"iframe\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Skip pages with very little text (e.g., login pages, error pages)\n",
    "    if len(text.split()) < 20:\n",
    "        return None\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def auto_crawl_and_index(\n",
    "    engine,\n",
    "    start_url: str,\n",
    "    max_pages: int = 50,\n",
    "    max_depth: int = 5,\n",
    "    delay: float = 1.0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fully automatic web crawler with indexing.\n",
    "    Starts from a seed URL and follows links breadth-first (BFS), just like Google.\n",
    "    Only crawls within the same domain and respects basic politeness rules.\n",
    "    \"\"\"\n",
    "    # Ensure URL has protocol\n",
    "    if not start_url.startswith(('http://', 'https://')):\n",
    "        start_url = 'https://' + start_url\n",
    "    \n",
    "    base_domain = urlparse(start_url).netloc\n",
    "    visited = set()\n",
    "    queue = deque([(start_url, 0)])  # (url, current_depth)\n",
    "\n",
    "    print(f\"Starting automatic crawl from: {start_url}\")\n",
    "    print(f\"Domain restricted to: {base_domain}\")\n",
    "    print(f\"Max pages: {max_pages}, Max depth: {max_depth}\\n\")\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        current_url, depth = queue.popleft()\n",
    "\n",
    "        # Skip if already visited or too deep\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        print(f\"[{len(visited)+1}/{max_pages}] Crawling: {current_url} (depth: {depth})\")\n",
    "\n",
    "        try:\n",
    "            # Polite User-Agent (important to avoid being blocked)\n",
    "            headers = {\n",
    "                'User-Agent': 'MyPersonalSearchBot/1.0 (+https://yourwebsite.com/bot-info)'\n",
    "            }\n",
    "            response = requests.get(current_url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Skip non-HTML content\n",
    "            content_type = response.headers.get('Content-Type', '')\n",
    "            if 'text/html' not in content_type:\n",
    "                print(f\"Not HTML, skipping\")\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            content = clean_content(response.text)\n",
    "            if content is None:\n",
    "                print(f\"  → Too little content, skipping\")\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            # Index the page into your SearchEngine\n",
    "            engine.index(current_url, content)\n",
    "            visited.add(current_url)\n",
    "\n",
    "            # Extract and enqueue new links\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            new_links = 0\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                next_url = urljoin(current_url, link['href'])\n",
    "\n",
    "                # Add only valid, unvisited internal links\n",
    "                if (is_valid_url(next_url, base_domain) and\n",
    "                    next_url not in visited and\n",
    "                    next_url not in [item[0] for item in queue]):\n",
    "                    \n",
    "                    queue.append((next_url, depth + 1))\n",
    "                    new_links += 1\n",
    "\n",
    "            print(f\"> Indexed successfully | Found {new_links} new links\")\n",
    "\n",
    "            # Be respectful: delay between requests\n",
    "            time.sleep(delay)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    print(f\"\\nCrawling finished! Total indexed pages: {len(visited)}\")\n",
    "    print(\"Your search engine is now ready to query these pages!\")\n",
    "\n",
    "\n",
    "# ==================== Example Usage ====================\n",
    "\n",
    "# Create your SearchEngine instance (make sure the class is defined earlier)\n",
    "engine = SearchEngine()\n",
    "\n",
    "# Start crawling from a seed page\n",
    "# Great test sites: small blogs, essay collections, documentation sites\n",
    "auto_crawl_and_index(\n",
    "    engine,\n",
    "    start_url=\"https://www.paulgraham.com/articles.html\",  # Paul Graham's essays (excellent content)\n",
    "    max_pages=30,\n",
    "    max_depth=3,\n",
    "    delay=1.5  # Be polite – 1.5 seconds between requests\n",
    ")\n",
    "\n",
    "# Test searches\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "results = engine.search(\"startup funding\", top_n=5)\n",
    "for rank, (url, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. [{score:.4f}] {url}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "results = engine.search(\"programming language design\")\n",
    "for rank, (url, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. [{score:.4f}] {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "from typing import Set\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "def is_valid_url(url: str, allowed_domains: Set[str] | None = None) -> bool:\n",
    "    \"\"\"\n",
    "    Validate URL: proper scheme, not a file, and (optionally) within allowed domains.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.scheme not in ('http', 'https'):\n",
    "        return False\n",
    "    if parsed.path.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.js', '.css', '.ico')):\n",
    "        return False\n",
    "    if allowed_domains is not None and parsed.netloc not in allowed_domains:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_content(html: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract clean readable text from HTML.\n",
    "    Removes noise (scripts, nav, footer, etc.) and skips pages with too little content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Remove non-content elements\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\", \n",
    "                     \"iframe\", \"noscript\", \"form\", \"button\", \"input\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Skip pages with almost no real content\n",
    "    if len(text.split()) < 30:\n",
    "        return None\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def get_robots_parser(domain: str, user_agent: str) -> RobotFileParser:\n",
    "    \"\"\"\n",
    "    Fetch and parse robots.txt for a given domain.\n",
    "    Returns a RobotFileParser object.\n",
    "    \"\"\"\n",
    "    rp = RobotFileParser()\n",
    "    robots_url = urllib.parse.urljoin(domain, '/robots.txt')\n",
    "    \n",
    "    try:\n",
    "        with urlopen(robots_url, timeout=10) as response:\n",
    "            rp.parse(response.read().decode('utf-8').splitlines())\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch robots.txt for {domain}: {e}\")\n",
    "        # If robots.txt is unavailable, assume all is allowed (common practice)\n",
    "    \n",
    "    return rp\n",
    "\n",
    "\n",
    "def auto_crawl_and_index(\n",
    "    engine,\n",
    "    seed_urls,\n",
    "    max_pages: int = 100,\n",
    "    max_depth: int = 6,\n",
    "    delay: float = 1.0,\n",
    "    global_mode: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Advanced automatic web crawler with two modes:\n",
    "    \n",
    "    - global_mode=False (default): Crawl within the initial domains only (deep dive one site)\n",
    "    - global_mode=True: Allow discovering and crawling NEW domains (true web-wide search engine behavior)\n",
    "    \n",
    "    Respects robots.txt for each domain.\n",
    "    \"\"\"\n",
    "    # Convert single string to list if needed\n",
    "    if isinstance(seed_urls, str):\n",
    "        seed_urls = [seed_urls]\n",
    "    \n",
    "    # Normalize seed URLs\n",
    "    normalized_seeds = []\n",
    "    for url in seed_urls:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url\n",
    "        normalized_seeds.append(url)\n",
    "    \n",
    "    seed_urls = normalized_seeds\n",
    "    \n",
    "    # Determine allowed domains\n",
    "    if global_mode:\n",
    "        allowed_domains = None  # No restriction - can explore the entire web\n",
    "        print(\"GLOBAL MODE ACTIVATED: Will discover and crawl new domains!\")\n",
    "    else:\n",
    "        allowed_domains = {urlparse(url).netloc for url in seed_urls}\n",
    "        print(f\"DOMAIN-LOCKED MODE: Limited to {len(allowed_domains)} starting domain(s)\")\n",
    "    \n",
    "    print(f\"Seed URLs: {seed_urls}\")\n",
    "    print(f\"Max pages: {max_pages} | Max depth: {max_depth} | Delay: {delay}s\\n\")\n",
    "\n",
    "    visited = set()\n",
    "    queue = deque([(url, 0) for url in seed_urls])  # (url, depth)\n",
    "\n",
    "    discovered_domains = set(urlparse(url).netloc for url in seed_urls)\n",
    "    robots_parsers = {}  # Cache: domain -> RobotFileParser\n",
    "\n",
    "    user_agent = 'MyGlobalSearchBot/1.0 (+https://yourwebsite.com/bot)'\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        current_url, depth = queue.popleft()\n",
    "\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        print(f\"[{len(visited)+1}/{max_pages}] Crawling: {current_url} (depth: {depth})\")\n",
    "\n",
    "        # Get domain and check robots.txt\n",
    "        parsed_url = urlparse(current_url)\n",
    "        domain = f\"{parsed_url.scheme}://{parsed_url.netloc}/\"\n",
    "        \n",
    "        if domain not in robots_parsers:\n",
    "            robots_parsers[domain] = get_robots_parser(domain, user_agent)\n",
    "        \n",
    "        rp = robots_parsers[domain]\n",
    "        \n",
    "        if not rp.can_fetch(user_agent, current_url):\n",
    "            print(f\"Blocked by robots.txt: {current_url}\")\n",
    "            visited.add(current_url)  # Mark as visited to avoid retrying\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            headers = {'User-Agent': user_agent}\n",
    "            response = requests.get(current_url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            content = clean_content(response.text)\n",
    "            if content is None:\n",
    "                visited.add(current_url)\n",
    "                continue\n",
    "\n",
    "            # Index the page\n",
    "            engine.index(current_url, content)\n",
    "            visited.add(current_url)\n",
    "\n",
    "            current_domain = parsed_url.netloc\n",
    "            if current_domain not in discovered_domains and global_mode:\n",
    "                discovered_domains.add(current_domain)\n",
    "                print(f\"NEW DOMAIN DISCOVERED: {current_domain}\")\n",
    "\n",
    "            # Extract links\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            new_links = 0\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                next_url = urljoin(current_url, link['href'])\n",
    "\n",
    "                if (is_valid_url(next_url, allowed_domains) and\n",
    "                    next_url not in visited and\n",
    "                    next_url not in [item[0] for item in queue]):\n",
    "\n",
    "                    queue.append((next_url, depth + 1))\n",
    "                    new_links += 1\n",
    "\n",
    "            print(f\"Indexed | +{new_links} new links queued\")\n",
    "\n",
    "            time.sleep(delay)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    print(f\"\\nCrawling complete! Indexed {len(visited)} pages.\")\n",
    "    if global_mode:\n",
    "        print(f\"Discovered {len(discovered_domains)} different domains.\")\n",
    "    print(\"Your personal search engine now knows more of the web!\")\n",
    "\n",
    "\n",
    "# ==================== USAGE EXAMPLES ====================\n",
    "\n",
    "engine = SearchEngine()  # Make sure your SearchEngine class is defined\n",
    "\n",
    "# 1. Single site deep crawl (original behavior)\n",
    "print(\"Example 1: Deep crawl of one site\")\n",
    "auto_crawl_and_index(\n",
    "    engine,\n",
    "    seed_urls=\"https://www.paulgraham.com/articles.html\",\n",
    "    max_pages=40,\n",
    "    max_depth=4,\n",
    "    delay=1.5,\n",
    "    global_mode=False\n",
    ")\n",
    "\n",
    "# 2. Global web exploration mode - discovers new sites!\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example 2: GLOBAL MODE - Exploring the open web\")\n",
    "auto_crawl_and_index(\n",
    "    engine,\n",
    "    seed_urls=[\n",
    "        \"https://news.ycombinator.com\",\n",
    "        \"https://www.lesswrong.com\",\n",
    "        \"https://www.paulgraham.com\"\n",
    "    ],\n",
    "    max_pages=80,\n",
    "    max_depth=5,\n",
    "    delay=2.0,          # Be extra polite in global mode\n",
    "    global_mode=True   # This is the key!\n",
    ")\n",
    "\n",
    "# Test your growing search engine\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "results = engine.search(\"artificial intelligence ethics\")\n",
    "for i, (url, score) in enumerate(results[:5], 1):\n",
    "    print(f\"{i}. [{score:.4f}] {url}\")\n",
    "\n",
    "results = engine.search(\"best programming books\")\n",
    "for i, (url, score) in enumerate(results[:5], 1):\n",
    "    print(f\"{i}. [{score:.4f}] {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6755a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling domain: convex-optimization-for-all.github.io\n",
      "Main URL: https://convex-optimization-for-all.github.io/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   3%|▎         | 1/30 [00:00<00:05,  5.27page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 248 URLs from sitemap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages: 260page [00:16, 15.47page/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV saved: convex_optimization_crawled.csv (260 rows)\n",
      "Finished. Indexed 260 pages.\n",
      "\n",
      "Example search results ('convex function'):\n",
      "3.1920 | https://convex-optimization-for-all.github.io/contents/chapter23/2021/03/28/23_01_Coordinate_descent/\n",
      "3.1854 | https://convex-optimization-for-all.github.io/contents/chapter15/2021/03/28/15_01_02_log_barrier_function_and_barrier_method/\n",
      "3.1407 | https://convex-optimization-for-all.github.io/contents/chapter03/2021/02/12/03_04_04_operations_that_preserve_quasiconvexity/\n",
      "3.1225 | https://convex-optimization-for-all.github.io/contents/chapter03/2021/02/12/03_06_convexity_with_respect_to_generalized_inequalities/\n",
      "3.0724 | https://convex-optimization-for-all.github.io/contents/chapter15/2021/03/28/15_01_01_inequality_constrained_minimization_problems/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import traceback\n",
    "\n",
    "def normalize_string(s: str) -> str:\n",
    "    \"\"\"Simple normalization: lowercase and strip whitespace\"\"\"\n",
    "    return s.lower().strip()\n",
    "\n",
    "UrlScores = Dict[str, float]\n",
    "\n",
    "def update_url_scores(existing: UrlScores, new_scores: UrlScores) -> None:\n",
    "    \"\"\"Add new scores to existing scores (sum)\"\"\"\n",
    "    for url, score in new_scores.items():\n",
    "        existing[url] = existing.get(url, 0.0) + score\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self._index = defaultdict(lambda: defaultdict(int))\n",
    "        self._documents = {}\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "    @property\n",
    "    def number_of_documents(self) -> int:\n",
    "        return len(self._documents)\n",
    "\n",
    "    @property\n",
    "    def avdl(self) -> float:\n",
    "        if not self._documents:\n",
    "            return 0.0\n",
    "        total_words = sum(len(content.split()) for content in self._documents.values())\n",
    "        return total_words / self.number_of_documents\n",
    "\n",
    "    def idf(self, kw: str) -> float:\n",
    "        N = self.number_of_documents\n",
    "        kw_norm = normalize_string(kw)\n",
    "        n_kw = len(self._index[kw_norm])\n",
    "        return log((N - n_kw + 0.5) / (n_kw + 0.5) + 1)\n",
    "\n",
    "    def bm25(self, kw: str) -> UrlScores:\n",
    "        kw_norm = normalize_string(kw)\n",
    "        if kw_norm not in self._index:\n",
    "            return {}\n",
    "        result = {}\n",
    "        idf_score = self.idf(kw)\n",
    "        avdl = self.avdl\n",
    "        for url, freq in self._index[kw_norm].items():\n",
    "            doc_length = len(self._documents[url].split())\n",
    "            numerator = freq * (self.k1 + 1)\n",
    "            denominator = freq + self.k1 * (1 - self.b + self.b * doc_length / avdl)\n",
    "            result[url] = idf_score * (numerator / denominator)\n",
    "        return result\n",
    "\n",
    "    def search(self, query: str, top_n: int = 5) -> List[Tuple[str, float]]:\n",
    "        keywords = normalize_string(query).split()\n",
    "        url_scores = {}\n",
    "        for kw in keywords:\n",
    "            kw_scores = self.bm25(kw)\n",
    "            update_url_scores(url_scores, kw_scores)\n",
    "        ranked = sorted(url_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_n]\n",
    "\n",
    "    def index(self, url: str, content: str) -> None:\n",
    "        self._documents[url] = content\n",
    "        words = normalize_string(content).split()\n",
    "        for word in words:\n",
    "            self._index[word][url] += 1\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    \"\"\"Extract domain (netloc) from URL\"\"\"\n",
    "    return urlparse(url).netloc\n",
    "\n",
    "def extract_links(content, base_url, allowed_domain):\n",
    "    \"\"\"Extract only links within the same domain (absolute + relative)\"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    links = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href'].strip()\n",
    "        if not href or href.startswith(('#', 'mailto:', 'tel:', 'javascript:')):\n",
    "            continue\n",
    "        full_url = urljoin(base_url, href)\n",
    "        parsed = urlparse(full_url)\n",
    "        if parsed.scheme not in ('http', 'https'):\n",
    "            continue\n",
    "        if get_domain(full_url) == allowed_domain:\n",
    "            links.append(full_url)\n",
    "    return list(set(links))[:10]  # Limit per page to avoid explosion\n",
    "\n",
    "def get_page_title_and_summary(content):\n",
    "    \"\"\"Extract page title and a short summary (meta or first paragraph)\"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    title = soup.title.string.strip() if soup.title else \"No title\"\n",
    "    meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "    if meta_desc and meta_desc.get('content'):\n",
    "        summary = meta_desc['content'][:200]\n",
    "    else:\n",
    "        first_p = soup.find('p')\n",
    "        summary = first_p.get_text().strip()[:200] if first_p else \"No summary\"\n",
    "    return title, summary\n",
    "\n",
    "def fetch_and_parse_sitemap(sitemap_url, allowed_domain, task_queue, visited, lock):\n",
    "    \"\"\"Fetch and parse sitemap.xml, add URLs to queue\"\"\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url, timeout=10, headers={'User-Agent': 'SimpleBFSBot/1.0'})\n",
    "        if response.status_code != 200:\n",
    "            return 0\n",
    "        \n",
    "        root = ET.fromstring(response.content)\n",
    "        namespace = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        \n",
    "        added = 0\n",
    "        # Handle <urlset>\n",
    "        for url_elem in root.findall('sitemap:url', namespace):\n",
    "            loc = url_elem.find('sitemap:loc', namespace)\n",
    "            if loc is not None and get_domain(loc.text) == allowed_domain:\n",
    "                with lock:\n",
    "                    if loc.text not in visited:\n",
    "                        task_queue.put(loc.text)\n",
    "                        added += 1\n",
    "        \n",
    "        # Handle <sitemapindex> (simple, no recursion)\n",
    "        for sitemap_elem in root.findall('sitemap:sitemap', namespace):\n",
    "            loc = sitemap_elem.find('sitemap:loc', namespace)\n",
    "            if loc is not None:\n",
    "                # Recurse for sub-sitemaps (but limit depth)\n",
    "                added += fetch_and_parse_sitemap(loc.text, allowed_domain, task_queue, visited, lock)\n",
    "        \n",
    "        return added\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing sitemap {sitemap_url}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def worker(task_queue, engine, visited, crawl_data, max_pages, rp, allowed_domain, lock, pbar, main_url):\n",
    "    while True:\n",
    "        try:\n",
    "            url = task_queue.get(timeout=1)  # Timeout to check for empty queue\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        \n",
    "        with lock:\n",
    "            if url in visited:\n",
    "                task_queue.task_done()\n",
    "                continue\n",
    "            visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            if not rp.can_fetch('*', url):\n",
    "                print(f\"Blocked by robots.txt: {url}\")\n",
    "                continue\n",
    "            \n",
    "            response = requests.get(url, timeout=12, headers={'User-Agent': 'SimpleBFSBot/1.0'})\n",
    "            if response.status_code != 200:\n",
    "                raise ValueError(f\"Status {response.status_code}\")\n",
    "            \n",
    "            content = response.text\n",
    "            engine.index(url, content)\n",
    "            \n",
    "            title, summary = get_page_title_and_summary(content)\n",
    "            \n",
    "            with lock:\n",
    "                crawl_data.append({\n",
    "                    'main_url': main_url,\n",
    "                    'url': url,\n",
    "                    'title': title,\n",
    "                    'summary': summary,\n",
    "                    'crawled_at': datetime.now().isoformat(),\n",
    "                    'status': 'success'\n",
    "                })\n",
    "                pbar.update(1)\n",
    "            \n",
    "            # Enqueue new links\n",
    "            new_urls = extract_links(content, url, allowed_domain)\n",
    "            for new_url in new_urls:\n",
    "                with lock:\n",
    "                    if new_url not in visited:\n",
    "                        task_queue.put(new_url)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error on {url}: {e}\")\n",
    "            with lock:\n",
    "                crawl_data.append({\n",
    "                    'main_url': main_url,\n",
    "                    'url': url,\n",
    "                    'title': 'Error',\n",
    "                    'summary': str(e)[:200],\n",
    "                    'crawled_at': datetime.now().isoformat(),\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "                if url in visited:\n",
    "                    visited.remove(url)  # Allow retry if needed\n",
    "        finally:\n",
    "            task_queue.task_done()\n",
    "\n",
    "# Multi-threaded BFS crawler with robots.txt, sitemap, progress, and CSV\n",
    "def run_crawler(initial_seeds, num_threads=4, max_pages=50, output_csv='crawled_pages.csv'):\n",
    "    if not initial_seeds:\n",
    "        print(\"No initial seeds provided.\")\n",
    "        return None\n",
    "\n",
    "    start_domain = get_domain(initial_seeds[0])\n",
    "    main_url = initial_seeds[0]\n",
    "    print(f\"Crawling domain: {start_domain}\")\n",
    "    print(f\"Main URL: {main_url}\")\n",
    "\n",
    "    task_queue = queue.Queue()\n",
    "    visited = set()\n",
    "    engine = SearchEngine()\n",
    "    crawl_data = []\n",
    "    lock = threading.Lock()\n",
    "    pbar = tqdm(total=max_pages, desc=\"Crawling pages\", unit=\"page\")\n",
    "\n",
    "    # Load robots.txt\n",
    "    rp = RobotFileParser()\n",
    "    robots_url = f\"https://{start_domain}/robots.txt\"\n",
    "    try:\n",
    "        response = requests.get(robots_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            rp.parse(response.text.splitlines())\n",
    "        else:\n",
    "            print(f\"No robots.txt found ({response.status_code}), assuming all allowed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching robots.txt: {e}, assuming all allowed.\")\n",
    "\n",
    "    # Load sitemap if exists\n",
    "    sitemap_url = f\"https://{start_domain}/sitemap.xml\"\n",
    "    added_from_sitemap = fetch_and_parse_sitemap(sitemap_url, start_domain, task_queue, visited, lock)\n",
    "    print(f\"Added {added_from_sitemap} URLs from sitemap.\")\n",
    "\n",
    "    # Enqueue initial seeds\n",
    "    for url in initial_seeds:\n",
    "        with lock:\n",
    "            if url not in visited:\n",
    "                task_queue.put(url)\n",
    "\n",
    "    # Start worker threads\n",
    "    threads = []\n",
    "    for _ in range(num_threads):\n",
    "        t = threading.Thread(target=worker, args=(task_queue, engine, visited, crawl_data, max_pages, rp, start_domain, lock, pbar, main_url))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # Wait for queue to empty\n",
    "    task_queue.join()\n",
    "\n",
    "    # Stop workers (not necessary with timeout, but clean)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Save to CSV\n",
    "    if crawl_data:\n",
    "        fieldnames = ['main_url', 'url', 'title', 'summary', 'crawled_at', 'status']\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(crawl_data)\n",
    "        print(f\"\\nCSV saved: {output_csv} ({len(crawl_data)} rows)\")\n",
    "    else:\n",
    "        print(\"No data collected.\")\n",
    "\n",
    "    print(f\"Finished. Indexed {engine.number_of_documents} pages.\")\n",
    "    \n",
    "    return engine\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    initial_seeds = [\n",
    "        'https://convex-optimization-for-all.github.io/'\n",
    "    ]\n",
    "    \n",
    "    engine = run_crawler(initial_seeds, num_threads=4, max_pages=30, output_csv='convex_optimization_crawled.csv')\n",
    "    \n",
    "    # Example search after crawling\n",
    "    if engine:\n",
    "        results = engine.search(\"convex function\", top_n=5)\n",
    "        print(\"\\nExample search results ('convex function'):\")\n",
    "        for url, score in results:\n",
    "            print(f\"{score:.4f} | {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4613cd2",
   "metadata": {},
   "source": [
    "### Save data to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling domain: convex-optimization-for-all.github.io\n",
      "Main URL: https://convex-optimization-for-all.github.io/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:   0%|          | 0/30 [00:00<?, ?page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 248 URLs from sitemap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages: 260page [00:11, 23.48page/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database saved: convex_optimization_crawled.db (260 rows)\n",
      "Finished. Indexed 260 pages.\n",
      "\n",
      "Example search results ('convex function'):\n",
      "3.1920 | https://convex-optimization-for-all.github.io/contents/chapter23/2021/03/28/23_01_Coordinate_descent/\n",
      "3.1854 | https://convex-optimization-for-all.github.io/contents/chapter15/2021/03/28/15_01_02_log_barrier_function_and_barrier_method/\n",
      "3.1407 | https://convex-optimization-for-all.github.io/contents/chapter03/2021/02/12/03_04_04_operations_that_preserve_quasiconvexity/\n",
      "3.1225 | https://convex-optimization-for-all.github.io/contents/chapter03/2021/02/12/03_06_convexity_with_respect_to_generalized_inequalities/\n",
      "3.0724 | https://convex-optimization-for-all.github.io/contents/chapter15/2021/03/28/15_01_01_inequality_constrained_minimization_problems/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from datetime import datetime\n",
    "import sqlite3  # Added for SQL storage\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import traceback\n",
    "\n",
    "def normalize_string(s: str) -> str:\n",
    "    \"\"\"Simple normalization: lowercase and strip whitespace\"\"\"\n",
    "    return s.lower().strip()\n",
    "\n",
    "UrlScores = Dict[str, float]\n",
    "\n",
    "def update_url_scores(existing: UrlScores, new_scores: UrlScores) -> None:\n",
    "    \"\"\"Add new scores to existing scores (sum)\"\"\"\n",
    "    for url, score in new_scores.items():\n",
    "        existing[url] = existing.get(url, 0.0) + score\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self._index = defaultdict(lambda: defaultdict(int))\n",
    "        self._documents = {}\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "    @property\n",
    "    def number_of_documents(self) -> int:\n",
    "        return len(self._documents)\n",
    "\n",
    "    @property\n",
    "    def avdl(self) -> float:\n",
    "        if not self._documents:\n",
    "            return 0.0\n",
    "        total_words = sum(len(content.split()) for content in self._documents.values())\n",
    "        return total_words / self.number_of_documents\n",
    "\n",
    "    def idf(self, kw: str) -> float:\n",
    "        N = self.number_of_documents\n",
    "        kw_norm = normalize_string(kw)\n",
    "        n_kw = len(self._index[kw_norm])\n",
    "        return log((N - n_kw + 0.5) / (n_kw + 0.5) + 1)\n",
    "\n",
    "    def bm25(self, kw: str) -> UrlScores:\n",
    "        kw_norm = normalize_string(kw)\n",
    "        if kw_norm not in self._index:\n",
    "            return {}\n",
    "        result = {}\n",
    "        idf_score = self.idf(kw)\n",
    "        avdl = self.avdl\n",
    "        for url, freq in self._index[kw_norm].items():\n",
    "            doc_length = len(self._documents[url].split())\n",
    "            numerator = freq * (self.k1 + 1)\n",
    "            denominator = freq + self.k1 * (1 - self.b + self.b * doc_length / avdl)\n",
    "            result[url] = idf_score * (numerator / denominator)\n",
    "        return result\n",
    "\n",
    "    def search(self, query: str, top_n: int = 5) -> List[Tuple[str, float]]:\n",
    "        keywords = normalize_string(query).split()\n",
    "        url_scores = {}\n",
    "        for kw in keywords:\n",
    "            kw_scores = self.bm25(kw)\n",
    "            update_url_scores(url_scores, kw_scores)\n",
    "        ranked = sorted(url_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_n]\n",
    "\n",
    "    def index(self, url: str, content: str) -> None:\n",
    "        self._documents[url] = content\n",
    "        words = normalize_string(content).split()\n",
    "        for word in words:\n",
    "            self._index[word][url] += 1\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    \"\"\"Extract domain (netloc) from URL\"\"\"\n",
    "    return urlparse(url).netloc\n",
    "\n",
    "def extract_links(content, base_url, allowed_domain):\n",
    "    \"\"\"Extract only links within the same domain (absolute + relative)\"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    links = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href'].strip()\n",
    "        if not href or href.startswith(('#', 'mailto:', 'tel:', 'javascript:')):\n",
    "            continue\n",
    "        full_url = urljoin(base_url, href)\n",
    "        parsed = urlparse(full_url)\n",
    "        if parsed.scheme not in ('http', 'https'):\n",
    "            continue\n",
    "        if get_domain(full_url) == allowed_domain:\n",
    "            links.append(full_url)\n",
    "    return list(set(links))[:10]  # Limit per page to avoid explosion\n",
    "\n",
    "def get_page_title_and_summary(content):\n",
    "    \"\"\"Extract page title and a short summary (meta or first paragraph)\"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    title = soup.title.string.strip() if soup.title else \"No title\"\n",
    "    meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "    if meta_desc and meta_desc.get('content'):\n",
    "        summary = meta_desc['content'][:200]\n",
    "    else:\n",
    "        first_p = soup.find('p')\n",
    "        summary = first_p.get_text().strip()[:200] if first_p else \"No summary\"\n",
    "    return title, summary\n",
    "\n",
    "def fetch_and_parse_sitemap(sitemap_url, allowed_domain, task_queue, visited, lock):\n",
    "    \"\"\"Fetch and parse sitemap.xml, add URLs to queue\"\"\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url, timeout=10, headers={'User-Agent': 'SimpleBFSBot/1.0'})\n",
    "        if response.status_code != 200:\n",
    "            return 0\n",
    "        \n",
    "        root = ET.fromstring(response.content)\n",
    "        namespace = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        \n",
    "        added = 0\n",
    "        # Handle <urlset>\n",
    "        for url_elem in root.findall('sitemap:url', namespace):\n",
    "            loc = url_elem.find('sitemap:loc', namespace)\n",
    "            if loc is not None and get_domain(loc.text) == allowed_domain:\n",
    "                with lock:\n",
    "                    if loc.text not in visited:\n",
    "                        task_queue.put(loc.text)\n",
    "                        added += 1\n",
    "        \n",
    "        # Handle <sitemapindex> (simple, no recursion)\n",
    "        for sitemap_elem in root.findall('sitemap:sitemap', namespace):\n",
    "            loc = sitemap_elem.find('sitemap:loc', namespace)\n",
    "            if loc is not None:\n",
    "                # Recurse for sub-sitemaps (but limit depth)\n",
    "                added += fetch_and_parse_sitemap(loc.text, allowed_domain, task_queue, visited, lock)\n",
    "        \n",
    "        return added\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing sitemap {sitemap_url}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def worker(task_queue, engine, visited, crawl_data, max_pages, rp, allowed_domain, lock, pbar, main_url):\n",
    "    while True:\n",
    "        try:\n",
    "            url = task_queue.get(timeout=1)  # Timeout to check for empty queue\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        \n",
    "        with lock:\n",
    "            if url in visited:\n",
    "                task_queue.task_done()\n",
    "                continue\n",
    "            visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            if not rp.can_fetch('*', url):\n",
    "                print(f\"Blocked by robots.txt: {url}\")\n",
    "                continue\n",
    "            \n",
    "            response = requests.get(url, timeout=12, headers={'User-Agent': 'SimpleBFSBot/1.0'})\n",
    "            if response.status_code != 200:\n",
    "                raise ValueError(f\"Status {response.status_code}\")\n",
    "            \n",
    "            content = response.text\n",
    "            engine.index(url, content)\n",
    "            \n",
    "            title, summary = get_page_title_and_summary(content)\n",
    "            \n",
    "            with lock:\n",
    "                crawl_data.append({\n",
    "                    'main_url': main_url,\n",
    "                    'url': url,\n",
    "                    'title': title,\n",
    "                    'summary': summary,\n",
    "                    'crawled_at': datetime.now().isoformat(),\n",
    "                    'status': 'success'\n",
    "                })\n",
    "                pbar.update(1)\n",
    "            \n",
    "            # Enqueue new links\n",
    "            new_urls = extract_links(content, url, allowed_domain)\n",
    "            for new_url in new_urls:\n",
    "                with lock:\n",
    "                    if new_url not in visited:\n",
    "                        task_queue.put(new_url)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error on {url}: {e}\")\n",
    "            with lock:\n",
    "                crawl_data.append({\n",
    "                    'main_url': main_url,\n",
    "                    'url': url,\n",
    "                    'title': 'Error',\n",
    "                    'summary': str(e)[:200],\n",
    "                    'crawled_at': datetime.now().isoformat(),\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "                if url in visited:\n",
    "                    visited.remove(url)  # Allow retry if needed\n",
    "        finally:\n",
    "            task_queue.task_done()\n",
    "\n",
    "# Multi-threaded BFS crawler with robots.txt, sitemap, progress, and SQL storage\n",
    "def run_crawler(initial_seeds, num_threads=4, max_pages=50, output_db='crawled_pages.db'):\n",
    "    if not initial_seeds:\n",
    "        print(\"No initial seeds provided.\")\n",
    "        return None\n",
    "\n",
    "    start_domain = get_domain(initial_seeds[0])\n",
    "    main_url = initial_seeds[0]\n",
    "    print(f\"Crawling domain: {start_domain}\")\n",
    "    print(f\"Main URL: {main_url}\")\n",
    "\n",
    "    task_queue = queue.Queue()\n",
    "    visited = set()\n",
    "    engine = SearchEngine()\n",
    "    crawl_data = []\n",
    "    lock = threading.Lock()\n",
    "    pbar = tqdm(total=max_pages, desc=\"Crawling pages\", unit=\"page\")\n",
    "\n",
    "    # Load robots.txt\n",
    "    rp = RobotFileParser()\n",
    "    robots_url = f\"https://{start_domain}/robots.txt\"\n",
    "    try:\n",
    "        response = requests.get(robots_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            rp.parse(response.text.splitlines())\n",
    "        else:\n",
    "            print(f\"No robots.txt found ({response.status_code}), assuming all allowed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching robots.txt: {e}, assuming all allowed.\")\n",
    "\n",
    "    # Load sitemap if exists\n",
    "    sitemap_url = f\"https://{start_domain}/sitemap.xml\"\n",
    "    added_from_sitemap = fetch_and_parse_sitemap(sitemap_url, start_domain, task_queue, visited, lock)\n",
    "    print(f\"Added {added_from_sitemap} URLs from sitemap.\")\n",
    "\n",
    "    # Enqueue initial seeds\n",
    "    for url in initial_seeds:\n",
    "        with lock:\n",
    "            if url not in visited:\n",
    "                task_queue.put(url)\n",
    "\n",
    "    # Start worker threads\n",
    "    threads = []\n",
    "    for _ in range(num_threads):\n",
    "        t = threading.Thread(target=worker, args=(task_queue, engine, visited, crawl_data, max_pages, rp, start_domain, lock, pbar, main_url))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # Wait for queue to empty\n",
    "    task_queue.join()\n",
    "\n",
    "    # Stop workers (not necessary with timeout, but clean)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Save to SQL (SQLite)\n",
    "    if crawl_data:\n",
    "        conn = sqlite3.connect(output_db)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create table if not exists\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS crawled_pages (\n",
    "                main_url TEXT,\n",
    "                url TEXT PRIMARY KEY,\n",
    "                title TEXT,\n",
    "                summary TEXT,\n",
    "                crawled_at TEXT,\n",
    "                status TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Insert data\n",
    "        for row in crawl_data:\n",
    "            cursor.execute('''\n",
    "                INSERT OR REPLACE INTO crawled_pages \n",
    "                (main_url, url, title, summary, crawled_at, status) \n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (row['main_url'], row['url'], row['title'], row['summary'], row['crawled_at'], row['status']))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(f\"\\nDatabase saved: {output_db} ({len(crawl_data)} rows)\")\n",
    "    else:\n",
    "        print(\"No data collected.\")\n",
    "\n",
    "    print(f\"Finished. Indexed {engine.number_of_documents} pages.\")\n",
    "    \n",
    "    return engine\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    initial_seeds = [\n",
    "        'https://convex-optimization-for-all.github.io/',\n",
    "        'https://naver.com/'\n",
    "    ]\n",
    "    \n",
    "    engine = run_crawler(initial_seeds, num_threads=4, max_pages=30, output_db='convex_optimization_crawled.db')\n",
    "    \n",
    "    # Example search after crawling\n",
    "    if engine:\n",
    "        results = engine.search(\"convex function\", top_n=5)\n",
    "        print(\"\\nExample search results ('convex function'):\")\n",
    "        for url, score in results:\n",
    "            print(f\"{score:.4f} | {url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_engine (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
